{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-26T05:24:45.798987Z",
     "iopub.status.busy": "2021-08-26T05:24:45.798249Z",
     "iopub.status.idle": "2021-08-26T05:24:47.037223Z",
     "shell.execute_reply": "2021-08-26T05:24:47.036374Z",
     "shell.execute_reply.started": "2021-08-26T03:55:40.361982Z"
    },
    "papermill": {
     "duration": 1.278779,
     "end_time": "2021-08-26T05:24:47.037410",
     "exception": false,
     "start_time": "2021-08-26T05:24:45.758631",
     "status": "completed"
    },
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}\n",
    "import pandas as pd\n",
    "\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T05:24:47.142587Z",
     "iopub.status.busy": "2021-08-26T05:24:47.108756Z",
     "iopub.status.idle": "2021-08-26T05:24:47.153709Z",
     "shell.execute_reply": "2021-08-26T05:24:47.152857Z",
     "shell.execute_reply.started": "2021-08-26T03:55:41.744001Z"
    },
    "papermill": {
     "duration": 0.092879,
     "end_time": "2021-08-26T05:24:47.153895",
     "exception": false,
     "start_time": "2021-08-26T05:24:47.061016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "# data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "data_dir = '/home/data/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv(data_dir+'train.csv')\n",
    "    test = pd.read_csv(data_dir+'/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    train.head()\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T05:24:47.205674Z",
     "iopub.status.busy": "2021-08-26T05:24:47.205015Z",
     "iopub.status.idle": "2021-08-26T06:08:16.637132Z",
     "shell.execute_reply": "2021-08-26T06:08:16.636371Z",
     "shell.execute_reply.started": "2021-08-26T03:55:41.826159Z"
    },
    "papermill": {
     "duration": 2609.46111,
     "end_time": "2021-08-26T06:08:16.637495",
     "exception": false,
     "start_time": "2021-08-26T05:24:47.176385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  7.6min\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:08:16.697217Z",
     "iopub.status.busy": "2021-08-26T06:08:16.696121Z",
     "iopub.status.idle": "2021-08-26T06:08:16.717500Z",
     "shell.execute_reply": "2021-08-26T06:08:16.716829Z",
     "shell.execute_reply.started": "2021-08-26T04:39:00.809230Z"
    },
    "papermill": {
     "duration": 0.05632,
     "end_time": "2021-08-26T06:08:16.717639",
     "exception": false,
     "start_time": "2021-08-26T06:08:16.661319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:08:16.776339Z",
     "iopub.status.busy": "2021-08-26T06:08:16.775594Z",
     "iopub.status.idle": "2021-08-26T06:08:16.798018Z",
     "shell.execute_reply": "2021-08-26T06:08:16.797391Z",
     "shell.execute_reply.started": "2021-08-26T04:39:00.839322Z"
    },
    "papermill": {
     "duration": 0.055891,
     "end_time": "2021-08-26T06:08:16.798165",
     "exception": false,
     "start_time": "2021-08-26T06:08:16.742274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:08:16.856700Z",
     "iopub.status.busy": "2021-08-26T06:08:16.856011Z",
     "iopub.status.idle": "2021-08-26T06:08:16.860836Z",
     "shell.execute_reply": "2021-08-26T06:08:16.860305Z",
     "shell.execute_reply.started": "2021-08-26T04:39:00.878555Z"
    },
    "papermill": {
     "duration": 0.037909,
     "end_time": "2021-08-26T06:08:16.860973",
     "exception": false,
     "start_time": "2021-08-26T06:08:16.823064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:08:16.920130Z",
     "iopub.status.busy": "2021-08-26T06:08:16.919431Z",
     "iopub.status.idle": "2021-08-26T06:08:19.242196Z",
     "shell.execute_reply": "2021-08-26T06:08:19.242734Z",
     "shell.execute_reply.started": "2021-08-26T04:39:00.890036Z"
    },
    "papermill": {
     "duration": 2.358301,
     "end_time": "2021-08-26T06:08:19.243131",
     "exception": false,
     "start_time": "2021-08-26T06:08:16.884830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv(data_dir+'/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:08:19.304978Z",
     "iopub.status.busy": "2021-08-26T06:08:19.304278Z",
     "iopub.status.idle": "2021-08-26T06:08:19.496365Z",
     "shell.execute_reply": "2021-08-26T06:08:19.496915Z",
     "shell.execute_reply.started": "2021-08-26T04:39:03.227624Z"
    },
    "papermill": {
     "duration": 0.227694,
     "end_time": "2021-08-26T06:08:19.497116",
     "exception": false,
     "start_time": "2021-08-26T06:08:19.269422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:08:19.562963Z",
     "iopub.status.busy": "2021-08-26T06:08:19.562015Z",
     "iopub.status.idle": "2021-08-26T06:08:28.062075Z",
     "shell.execute_reply": "2021-08-26T06:08:28.061451Z",
     "shell.execute_reply.started": "2021-08-26T04:39:03.433782Z"
    },
    "papermill": {
     "duration": 8.539365,
     "end_time": "2021-08-26T06:08:28.062214",
     "exception": false,
     "start_time": "2021-08-26T06:08:19.522849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] \n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:08:28.236232Z",
     "iopub.status.busy": "2021-08-26T06:08:28.235548Z",
     "iopub.status.idle": "2021-08-26T06:08:28.240286Z",
     "shell.execute_reply": "2021-08-26T06:08:28.239735Z",
     "shell.execute_reply.started": "2021-08-26T04:39:12.055844Z"
    },
    "papermill": {
     "duration": 0.150837,
     "end_time": "2021-08-26T06:08:28.240434",
     "exception": false,
     "start_time": "2021-08-26T06:08:28.089597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:08:28.313927Z",
     "iopub.status.busy": "2021-08-26T06:08:28.313190Z",
     "iopub.status.idle": "2021-08-26T06:18:20.671831Z",
     "shell.execute_reply": "2021-08-26T06:18:20.672369Z",
     "shell.execute_reply.started": "2021-08-26T04:39:12.232241Z"
    },
    "papermill": {
     "duration": 592.405766,
     "end_time": "2021-08-26T06:18:20.672566",
     "exception": false,
     "start_time": "2021-08-26T06:08:28.266800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428698\ttraining's RMSPE: 0.198264\tvalid_1's rmse: 0.00043934\tvalid_1's RMSPE: 0.203917\n",
      "[500]\ttraining's rmse: 0.000406922\ttraining's RMSPE: 0.188193\tvalid_1's rmse: 0.000425343\tvalid_1's RMSPE: 0.19742\n",
      "[750]\ttraining's rmse: 0.000392958\ttraining's RMSPE: 0.181735\tvalid_1's rmse: 0.000416835\tvalid_1's RMSPE: 0.193471\n",
      "[1000]\ttraining's rmse: 0.000383231\ttraining's RMSPE: 0.177236\tvalid_1's rmse: 0.000412414\tvalid_1's RMSPE: 0.191419\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000383231\ttraining's RMSPE: 0.177236\tvalid_1's rmse: 0.000412414\tvalid_1's RMSPE: 0.191419\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428653\ttraining's RMSPE: 0.19859\tvalid_1's rmse: 0.000440463\tvalid_1's RMSPE: 0.203012\n",
      "[500]\ttraining's rmse: 0.000406379\ttraining's RMSPE: 0.188271\tvalid_1's rmse: 0.000425018\tvalid_1's RMSPE: 0.195894\n",
      "[750]\ttraining's rmse: 0.000392668\ttraining's RMSPE: 0.181918\tvalid_1's rmse: 0.000417108\tvalid_1's RMSPE: 0.192248\n",
      "[1000]\ttraining's rmse: 0.000382831\ttraining's RMSPE: 0.177361\tvalid_1's rmse: 0.000412924\tvalid_1's RMSPE: 0.19032\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000382831\ttraining's RMSPE: 0.177361\tvalid_1's rmse: 0.000412924\tvalid_1's RMSPE: 0.19032\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.00042857\ttraining's RMSPE: 0.198198\tvalid_1's rmse: 0.000465999\tvalid_1's RMSPE: 0.21632\n",
      "[500]\ttraining's rmse: 0.000406477\ttraining's RMSPE: 0.187981\tvalid_1's rmse: 0.000453135\tvalid_1's RMSPE: 0.210349\n",
      "[750]\ttraining's rmse: 0.000392795\ttraining's RMSPE: 0.181653\tvalid_1's rmse: 0.000445963\tvalid_1's RMSPE: 0.207019\n",
      "[1000]\ttraining's rmse: 0.000383092\ttraining's RMSPE: 0.177166\tvalid_1's rmse: 0.000441162\tvalid_1's RMSPE: 0.204791\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000383092\ttraining's RMSPE: 0.177166\tvalid_1's rmse: 0.000441162\tvalid_1's RMSPE: 0.204791\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.00042854\ttraining's RMSPE: 0.198538\tvalid_1's rmse: 0.0004457\tvalid_1's RMSPE: 0.205425\n",
      "[500]\ttraining's rmse: 0.000406238\ttraining's RMSPE: 0.188205\tvalid_1's rmse: 0.00043002\tvalid_1's RMSPE: 0.198198\n",
      "[750]\ttraining's rmse: 0.000392756\ttraining's RMSPE: 0.18196\tvalid_1's rmse: 0.000422586\tvalid_1's RMSPE: 0.194771\n",
      "[1000]\ttraining's rmse: 0.000382971\ttraining's RMSPE: 0.177426\tvalid_1's rmse: 0.000418005\tvalid_1's RMSPE: 0.19266\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000382971\ttraining's RMSPE: 0.177426\tvalid_1's rmse: 0.000418005\tvalid_1's RMSPE: 0.19266\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429751\ttraining's RMSPE: 0.198779\tvalid_1's rmse: 0.000442733\tvalid_1's RMSPE: 0.205379\n",
      "[500]\ttraining's rmse: 0.000407766\ttraining's RMSPE: 0.188609\tvalid_1's rmse: 0.000428896\tvalid_1's RMSPE: 0.19896\n",
      "[750]\ttraining's rmse: 0.000394267\ttraining's RMSPE: 0.182366\tvalid_1's rmse: 0.000421757\tvalid_1's RMSPE: 0.195648\n",
      "[1000]\ttraining's rmse: 0.000383985\ttraining's RMSPE: 0.17761\tvalid_1's rmse: 0.000417711\tvalid_1's RMSPE: 0.193771\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000383985\ttraining's RMSPE: 0.17761\tvalid_1's rmse: 0.000417711\tvalid_1's RMSPE: 0.193771\n",
      "Our out of folds RMSPE is 0.19466237751890056\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1000,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb= train_and_evaluate_lgb(train, test,params0)\n",
    "test['target'] = predictions_lgb\n",
    "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:18:20.751475Z",
     "iopub.status.busy": "2021-08-26T06:18:20.750841Z",
     "iopub.status.idle": "2021-08-26T06:18:20.756068Z",
     "shell.execute_reply": "2021-08-26T06:18:20.756571Z",
     "shell.execute_reply.started": "2021-08-26T04:48:53.093706Z"
    },
    "papermill": {
     "duration": 0.045884,
     "end_time": "2021-08-26T06:18:20.756758",
     "exception": false,
     "start_time": "2021-08-26T06:18:20.710874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:18:20.835707Z",
     "iopub.status.busy": "2021-08-26T06:18:20.835023Z",
     "iopub.status.idle": "2021-08-26T06:18:27.073367Z",
     "shell.execute_reply": "2021-08-26T06:18:27.073930Z",
     "shell.execute_reply.started": "2021-08-26T04:48:53.101910Z"
    },
    "papermill": {
     "duration": 6.27956,
     "end_time": "2021-08-26T06:18:27.074106",
     "exception": false,
     "start_time": "2021-08-26T06:18:20.794546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:18:27.153468Z",
     "iopub.status.busy": "2021-08-26T06:18:27.152798Z",
     "iopub.status.idle": "2021-08-26T06:18:39.694354Z",
     "shell.execute_reply": "2021-08-26T06:18:39.694898Z",
     "shell.execute_reply.started": "2021-08-26T05:02:23.375706Z"
    },
    "papermill": {
     "duration": 12.582852,
     "end_time": "2021-08-26T06:18:39.695200",
     "exception": false,
     "start_time": "2021-08-26T06:18:27.112348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "\n",
    "out_train = pd.read_csv(data_dir+'/train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "out_train.head()\n",
    "\n",
    "# code to add the just the read data after first execution\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "\n",
    "\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):\n",
    "    \n",
    "    values.append([lineNumber[n]])    \n",
    "\n",
    "\n",
    "s=[]\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    \n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "\n",
    "    luck = np.random.uniform(0,1,nfolds)\n",
    "    \n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "\n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            \n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:18:39.774942Z",
     "iopub.status.busy": "2021-08-26T06:18:39.774193Z",
     "iopub.status.idle": "2021-08-26T06:19:08.926156Z",
     "shell.execute_reply": "2021-08-26T06:19:08.925300Z",
     "shell.execute_reply.started": "2021-08-26T05:04:30.178233Z"
    },
    "papermill": {
     "duration": 29.192845,
     "end_time": "2021-08-26T06:19:08.926372",
     "exception": false,
     "start_time": "2021-08-26T06:18:39.733527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:19:09.026881Z",
     "iopub.status.busy": "2021-08-26T06:19:09.025433Z",
     "iopub.status.idle": "2021-08-26T06:19:09.035235Z",
     "shell.execute_reply": "2021-08-26T06:19:09.034622Z",
     "shell.execute_reply.started": "2021-08-26T05:04:59.298556Z"
    },
    "papermill": {
     "duration": 0.05805,
     "end_time": "2021-08-26T06:19:09.035391",
     "exception": false,
     "start_time": "2021-08-26T06:19:08.977341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:19:09.127080Z",
     "iopub.status.busy": "2021-08-26T06:19:09.126313Z",
     "iopub.status.idle": "2021-08-26T06:19:10.936080Z",
     "shell.execute_reply": "2021-08-26T06:19:10.935186Z",
     "shell.execute_reply.started": "2021-08-26T05:04:59.318065Z"
    },
    "papermill": {
     "duration": 1.862292,
     "end_time": "2021-08-26T06:19:10.936316",
     "exception": false,
     "start_time": "2021-08-26T06:19:09.074024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv(data_dir+'/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:19:11.023764Z",
     "iopub.status.busy": "2021-08-26T06:19:11.023088Z",
     "iopub.status.idle": "2021-08-26T06:19:11.026332Z",
     "shell.execute_reply": "2021-08-26T06:19:11.025673Z",
     "shell.execute_reply.started": "2021-08-26T05:05:08.044692Z"
    },
    "papermill": {
     "duration": 0.049441,
     "end_time": "2021-08-26T06:19:11.026471",
     "exception": false,
     "start_time": "2021-08-26T06:19:10.977030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:19:11.118188Z",
     "iopub.status.busy": "2021-08-26T06:19:11.116067Z",
     "iopub.status.idle": "2021-08-26T06:19:11.270250Z",
     "shell.execute_reply": "2021-08-26T06:19:11.269487Z",
     "shell.execute_reply.started": "2021-08-26T05:05:10.837760Z"
    },
    "papermill": {
     "duration": 0.202899,
     "end_time": "2021-08-26T06:19:11.270404",
     "exception": false,
     "start_time": "2021-08-26T06:19:11.067505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:19:11.364672Z",
     "iopub.status.busy": "2021-08-26T06:19:11.363774Z",
     "iopub.status.idle": "2021-08-26T06:19:17.256463Z",
     "shell.execute_reply": "2021-08-26T06:19:17.255790Z",
     "shell.execute_reply.started": "2021-08-26T05:05:13.957209Z"
    },
    "papermill": {
     "duration": 5.943356,
     "end_time": "2021-08-26T06:19:17.256604",
     "exception": false,
     "start_time": "2021-08-26T06:19:11.313248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "del mat1,mat2\n",
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:19:17.347165Z",
     "iopub.status.busy": "2021-08-26T06:19:17.346464Z",
     "iopub.status.idle": "2021-08-26T06:19:17.363134Z",
     "shell.execute_reply": "2021-08-26T06:19:17.362409Z",
     "shell.execute_reply.started": "2021-08-26T05:07:40.604977Z"
    },
    "papermill": {
     "duration": 0.066974,
     "end_time": "2021-08-26T06:19:17.363278",
     "exception": false,
     "start_time": "2021-08-26T06:19:17.296304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(244,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:19:17.450265Z",
     "iopub.status.busy": "2021-08-26T06:19:17.449539Z",
     "iopub.status.idle": "2021-08-26T06:19:17.452778Z",
     "shell.execute_reply": "2021-08-26T06:19:17.452127Z",
     "shell.execute_reply.started": "2021-08-26T05:07:19.618669Z"
    },
    "papermill": {
     "duration": 0.049732,
     "end_time": "2021-08-26T06:19:17.452930",
     "exception": false,
     "start_time": "2021-08-26T06:19:17.403198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:19:17.555226Z",
     "iopub.status.busy": "2021-08-26T06:19:17.554486Z",
     "iopub.status.idle": "2021-08-26T06:34:37.560388Z",
     "shell.execute_reply": "2021-08-26T06:34:37.559837Z",
     "shell.execute_reply.started": "2021-08-26T05:07:43.746892Z"
    },
    "papermill": {
     "duration": 920.067309,
     "end_time": "2021-08-26T06:34:37.560537",
     "exception": false,
     "start_time": "2021-08-26T06:19:17.493228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 4s 19ms/step - loss: 23.6953 - val_loss: 1.7192\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.9857 - val_loss: 0.5698\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6524 - val_loss: 0.5460\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6289 - val_loss: 0.7143\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5223 - val_loss: 0.7325\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.9121 - val_loss: 0.7280\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6053 - val_loss: 0.5918\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5474 - val_loss: 0.5482\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5969 - val_loss: 0.6247\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6996 - val_loss: 0.3952\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4268 - val_loss: 0.5293\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4212 - val_loss: 0.4298\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4442 - val_loss: 0.3448\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3713 - val_loss: 0.2358\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2307 - val_loss: 0.2692\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2302 - val_loss: 0.2331\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2391 - val_loss: 0.2683\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.4012 - val_loss: 0.2457\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2456 - val_loss: 0.2255\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2290 - val_loss: 0.2152\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2257 - val_loss: 0.2235\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2268 - val_loss: 0.2654\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2242 - val_loss: 0.2178\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2210 - val_loss: 0.2153\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2248 - val_loss: 0.2362\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2332 - val_loss: 0.2676\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2474 - val_loss: 0.2359\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2137 - val_loss: 0.2093\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2099\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2095 - val_loss: 0.2107\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2099 - val_loss: 0.2101\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2109\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2091\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2139\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2122\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2095 - val_loss: 0.2104\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2084 - val_loss: 0.2096\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2098 - val_loss: 0.2090\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2094\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2097 - val_loss: 0.2121\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2065 - val_loss: 0.2088\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2087\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2091\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2069 - val_loss: 0.2090\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2082\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2069 - val_loss: 0.2081\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2093\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2100\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2087\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2057 - val_loss: 0.2101\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2069 - val_loss: 0.2092\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2084\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2102\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2054 - val_loss: 0.2081\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2084\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2084\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2083\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2084\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2088\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2087\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2082\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2081\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2081\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2036 - val_loss: 0.2081\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2082\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2043 - val_loss: 0.2081\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2036 - val_loss: 0.2081\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2050 - val_loss: 0.2081\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2033 - val_loss: 0.2081\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2082\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2081\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2033 - val_loss: 0.2083\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2082\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2082\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2081\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2042 - val_loss: 0.2081\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2081\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2081\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 3s 20ms/step - loss: 0.2044 - val_loss: 0.2081\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2035 - val_loss: 0.2081\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 4s 22ms/step - loss: 0.2039 - val_loss: 0.2081\n",
      "Fold 1 NN: 0.20807\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 25.6566 - val_loss: 1.6543\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.1011 - val_loss: 0.9733\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8101 - val_loss: 1.2899\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.1323 - val_loss: 0.8391\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.8537 - val_loss: 0.5552\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7836 - val_loss: 0.7285\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6763 - val_loss: 1.1901\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.0350 - val_loss: 0.2397\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2926 - val_loss: 0.2799\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3044 - val_loss: 0.2815\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3052 - val_loss: 0.3467\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3394 - val_loss: 0.2916\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3152 - val_loss: 0.3920\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4840 - val_loss: 5.6690\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 3.2251 - val_loss: 0.2610\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2268 - val_loss: 0.2277\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2206 - val_loss: 0.2262\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2161 - val_loss: 0.2270\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2171 - val_loss: 0.2225\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2150 - val_loss: 0.2215\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2140 - val_loss: 0.2215\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2267\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2149 - val_loss: 0.2186\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2127 - val_loss: 0.2189\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2197\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2132 - val_loss: 0.2180\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2124 - val_loss: 0.2201\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2131 - val_loss: 0.2214\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2119 - val_loss: 0.2199\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2246\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2146 - val_loss: 0.2200\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2139 - val_loss: 0.2181\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2126 - val_loss: 0.2202\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2071 - val_loss: 0.2153\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2147\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2145\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2157\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2157\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2136\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2052 - val_loss: 0.2136\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2063 - val_loss: 0.2154\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2054 - val_loss: 0.2147\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2165\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2153\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2130\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2158\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2054 - val_loss: 0.2130\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2060 - val_loss: 0.2159\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2082 - val_loss: 0.2135\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2044 - val_loss: 0.2181\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2129\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2169\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2049 - val_loss: 0.2176\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2132\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2148\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2042 - val_loss: 0.2161\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2147\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2205\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2023 - val_loss: 0.2137\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2136\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2136\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2116\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2020 - val_loss: 0.2138\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2013 - val_loss: 0.2138\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2134\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2023 - val_loss: 0.2137\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2135\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2019 - val_loss: 0.2135\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2117\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2126\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2128\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2125\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2004 - val_loss: 0.2128\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2123\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2129\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2131\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2005 - val_loss: 0.2128\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2007 - val_loss: 0.2128\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2126\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2004 - val_loss: 0.2129\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2008 - val_loss: 0.2128\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2130\n",
      "Fold 2 NN: 0.2116\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 25.1836 - val_loss: 0.8318\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6045 - val_loss: 0.5699\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5980 - val_loss: 0.3885\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6727 - val_loss: 0.4867\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5336 - val_loss: 0.5026\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4848 - val_loss: 0.4726\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5260 - val_loss: 0.4488\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.4300 - val_loss: 0.2252\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2468 - val_loss: 0.6261\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.8201 - val_loss: 0.2895\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3098 - val_loss: 0.2348\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2461 - val_loss: 0.2219\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2338 - val_loss: 0.2568\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2368 - val_loss: 0.3503\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.7626 - val_loss: 0.3276\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2574 - val_loss: 0.2317\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2356 - val_loss: 0.2164\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2248 - val_loss: 0.2413\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2272 - val_loss: 0.2952\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2360 - val_loss: 0.2352\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2350 - val_loss: 0.2624\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2335 - val_loss: 0.2246\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2406 - val_loss: 0.2680\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2345 - val_loss: 0.2121\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2274 - val_loss: 0.2292\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2444 - val_loss: 0.2420\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2369 - val_loss: 0.2936\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2360 - val_loss: 0.2229\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3068 - val_loss: 0.2209\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2415 - val_loss: 0.2167\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2225 - val_loss: 0.2460\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2112\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2112\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2103\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2119\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2052 - val_loss: 0.2094\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2046 - val_loss: 0.2094\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2042 - val_loss: 0.2098\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2098\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2113\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2105\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2100\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2126\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2021 - val_loss: 0.2091\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2093\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2098\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2095\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2093\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2094\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2093\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2014 - val_loss: 0.2098\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2010 - val_loss: 0.2091\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2019 - val_loss: 0.2093\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2093\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2003 - val_loss: 0.2091\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2006 - val_loss: 0.2092\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2011 - val_loss: 0.2095\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2095\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2092\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2008 - val_loss: 0.2092\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2091\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2012 - val_loss: 0.2092\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2091\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1999 - val_loss: 0.2092\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2007 - val_loss: 0.2091\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2011 - val_loss: 0.2092\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2011 - val_loss: 0.2092\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1998 - val_loss: 0.2093\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2007 - val_loss: 0.2092\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2008 - val_loss: 0.2092\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1994 - val_loss: 0.2092\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2006 - val_loss: 0.2092\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2003 - val_loss: 0.2092\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2004 - val_loss: 0.2092\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2001 - val_loss: 0.2092\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1997 - val_loss: 0.2092\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2006 - val_loss: 0.2092\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2003 - val_loss: 0.2092\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2007 - val_loss: 0.2092\n",
      "Fold 3 NN: 0.20909\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 21.0162 - val_loss: 1.3988\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8092 - val_loss: 0.5916\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6937 - val_loss: 0.4825\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.7655 - val_loss: 0.8508\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8862 - val_loss: 0.5065\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6349 - val_loss: 1.6054\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.0557 - val_loss: 0.2387\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2608 - val_loss: 0.2433\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2572 - val_loss: 0.2786\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2551 - val_loss: 0.2766\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2777 - val_loss: 0.2326\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2827 - val_loss: 0.2890\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 1.4429 - val_loss: 0.2607\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2625 - val_loss: 0.2354\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2347 - val_loss: 0.3021\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2314 - val_loss: 0.2304\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2401 - val_loss: 0.2480\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2352 - val_loss: 0.2200\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2412 - val_loss: 0.2533\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2361 - val_loss: 0.3906\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2492 - val_loss: 0.2280\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2424 - val_loss: 0.2520\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2354 - val_loss: 0.2253\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2500 - val_loss: 0.2363\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2405 - val_loss: 0.2754\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2151 - val_loss: 0.2162\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2059 - val_loss: 0.2175\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2060 - val_loss: 0.2172\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2067 - val_loss: 0.2171\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2066 - val_loss: 0.2183\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2150\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2173\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2054 - val_loss: 0.2202\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2066 - val_loss: 0.2210\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2051 - val_loss: 0.2204\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2068 - val_loss: 0.2182\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2102 - val_loss: 0.2169\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2082 - val_loss: 0.2170\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2032 - val_loss: 0.2154\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2153\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2030 - val_loss: 0.2152\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2025 - val_loss: 0.2169\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2028 - val_loss: 0.2158\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2024 - val_loss: 0.2170\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2021 - val_loss: 0.2160\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2023 - val_loss: 0.2158\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2159\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2160\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2156\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2018 - val_loss: 0.2159\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2011 - val_loss: 0.2156\n",
      "Fold 4 NN: 0.21499\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 32.7954 - val_loss: 0.9868\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.0166 - val_loss: 0.6044\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5945 - val_loss: 0.4791\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5925 - val_loss: 0.6527\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.5912 - val_loss: 0.7418\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4774 - val_loss: 0.2477\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2717 - val_loss: 0.2551\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2943 - val_loss: 0.2622\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3032 - val_loss: 0.2543\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3138 - val_loss: 0.3060\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3080 - val_loss: 1.2261\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 5.7817 - val_loss: 0.3898\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4181 - val_loss: 0.2534\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2281 - val_loss: 0.2269\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2222 - val_loss: 0.2264\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2210 - val_loss: 0.2256\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2203 - val_loss: 0.2251\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2194 - val_loss: 0.2236\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2172 - val_loss: 0.2251\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2175 - val_loss: 0.2251\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2163 - val_loss: 0.2309\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2175 - val_loss: 0.2212\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2160 - val_loss: 0.2225\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2160 - val_loss: 0.2213\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2175 - val_loss: 0.2268\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2172 - val_loss: 0.2233\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2181 - val_loss: 0.2310\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2173 - val_loss: 0.2201\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2155 - val_loss: 0.2221\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2146 - val_loss: 0.2227\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2233\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2151 - val_loss: 0.2193\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2128 - val_loss: 0.2395\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2213 - val_loss: 0.2242\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2242 - val_loss: 0.2180\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2119 - val_loss: 0.2575\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2216 - val_loss: 0.2241\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2121 - val_loss: 0.2301\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2131 - val_loss: 0.2410\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 3s 20ms/step - loss: 0.2158 - val_loss: 0.2295\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 4s 21ms/step - loss: 0.2167 - val_loss: 0.2746\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2155 - val_loss: 0.2186\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2162\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2158\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2156\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2168\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2049 - val_loss: 0.2157\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2228\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2052 - val_loss: 0.2200\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2170\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2043 - val_loss: 0.2148\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2041 - val_loss: 0.2161\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2049 - val_loss: 0.2158\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2182\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2169\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2189\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2151\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2160\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2157\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2160\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2018 - val_loss: 0.2154\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2023 - val_loss: 0.2149\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2015 - val_loss: 0.2159\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2149\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2148\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2015 - val_loss: 0.2150\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2016 - val_loss: 0.2148\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2018 - val_loss: 0.2147\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2019 - val_loss: 0.2151\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2012 - val_loss: 0.2147\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2149\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2011 - val_loss: 0.2148\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2026 - val_loss: 0.2149\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2148\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2149\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2149\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2148\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2149\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2149\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2015 - val_loss: 0.2148\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2009 - val_loss: 0.2148\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2013 - val_loss: 0.2148\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2024 - val_loss: 0.2149\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2015 - val_loss: 0.2149\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2014 - val_loss: 0.2149\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2148\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2013 - val_loss: 0.2149\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2148\n",
      "Fold 5 NN: 0.21466\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-26T06:34:46.304368Z",
     "iopub.status.busy": "2021-08-26T06:34:46.303647Z",
     "iopub.status.idle": "2021-08-26T06:34:46.339368Z",
     "shell.execute_reply": "2021-08-26T06:34:46.337779Z",
     "shell.execute_reply.started": "2021-08-26T05:24:19.999650Z"
    },
    "papermill": {
     "duration": 4.458621,
     "end_time": "2021-08-26T06:34:46.339576",
     "exception": false,
     "start_time": "2021-08-26T06:34:41.880955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSPE NN: 1.0 - Folds: [0.20807, 0.2116, 0.20909, 0.21499, 0.21466]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.001979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.001979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001626\n",
       "1   0-32  0.001979\n",
       "2   0-34  0.001979"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str) \n",
    "test_nn[target_name] = (test_predictions_nn+predictions_lgb)/2\n",
    "\n",
    "score = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\n",
    "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
    "\n",
    "display(test_nn[['row_id', target_name]].head(3))\n",
    "test_nn[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#kmeans N=5 [0.2101, 0.21399, 0.20923, 0.21398, 0.21175]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4215.959346,
   "end_time": "2021-08-26T06:34:53.123168",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-26T05:24:37.163822",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}