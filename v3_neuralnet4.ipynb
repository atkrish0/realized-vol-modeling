{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO1upiHT1011"
      },
      "source": [
        "PURPOSE: REMOVE SINGLE STOCK MENTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IysNA2h61bz2"
      },
      "source": [
        "[REF](https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/279170).\n",
        "\n",
        "Here I also include 2 modes of training: *single-stock* and *multi-stock*. *multi-stock* training works really fast and you can get decent results after 20 minutes. In this mode train batch includes targets for all stocks in time_id and you have only 3830 training samples. In *single-stock* mode input is still the same and includes data from all stocks, but it also has single stock_id as input and only single stock target for it is predicted. This way batch contains much more diverse (stock_id, time_id) pairs, and I believe this diversity is important to get better score.\n",
        "\n",
        "Future Scope:\n",
        "- stock attention placement (before/after RNN and internal implementation\n",
        "- feature normalization (something with volumes),\n",
        "- network dimensions, batch size, lr, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa8hBEYiQaOA"
      },
      "source": [
        "**Model Summary:**\n",
        "\n",
        "1.\tConv1D(21→32, k=3, s=3)\n",
        "•\tWhy? Learns local time-window patterns (e.g. short-term momentum or spread shifts) while immediately coarsening 600→200.\n",
        "2.\tGELU → LayerNorm\n",
        "•\tWhy? Smooth nonlinearity + per-stock+channel normalization to stabilize feature distributions before/after convolution.\n",
        "3.\tConv1D(32→32, k=1)\n",
        "•\tWhy? A “pointwise” mixing of the 32 channels, analogous to a 1×1 conv in vision models. Refines features at each time step.\n",
        "4.\tStockAttention\n",
        "•\tWhy? Learns which stocks’ 200×32 representations should inform one another in this time bucket.\n",
        "•\tA full 112×112 weight matrix allows every stock’s latent representation to borrow information from every other.\n",
        "5.\tStock Embedding\n",
        "•\tWhy? Provides a learned “bias” per stock so the network can distinguish them beyond their raw order-book features.\n",
        "6.\tGRU (32→32)\n",
        "•\tWhy? Captures the sequential dynamics within each stock’s 200-step series, integrating patterns over time.\n",
        "7.\tTimeAttention\n",
        "•\tWhy? Rather than simply averaging or taking the final hidden state, the model learns to focus on those time steps most predictive of realized volatility.\n",
        "8.\tLinear → Affine → exp\n",
        "•\tWhy? Projects the 32-dim attended vector down to a single log-volatility score, rescales (to match data distribution), and exponentiates back to the volatility domain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LngrRcFpWDfk"
      },
      "source": [
        "[ INPUT ]\n",
        " • X shape = (B, S=112, T′=⌊600/coarsen⌋, F=21)\n",
        " • raw book & trade features + engineered waps/log-returns/spreads/imbalances  \n",
        " • each feature standardized per stock\n",
        "\n",
        "\n",
        "[ Conv1d #1 ]\n",
        " • in_channels = 21  \n",
        " • out_channels = D  (e.g. 32)  \n",
        " • kernel = k₁ (e.g. 3), stride = s₁ (e.g. 3)  \n",
        " → output shape = (B, S, T₁=⌊T′/s₁⌋, D)\n",
        "\n",
        "\n",
        "[ GELU activation ]\n",
        "\n",
        "\n",
        "[ LayerNorm over (stocks S, features D) ]\n",
        " • normalize across dims 2 & 3  \n",
        " → (B, S, T₁, D)\n",
        "\n",
        "\n",
        "[ Conv1d #2 ]\n",
        " • in_channels = D → out_channels = D  \n",
        " • kernel = 1, stride = 1  \n",
        " → (B, S, T₁, D)\n",
        "\n",
        "\n",
        "[ GELU activation ]\n",
        "\n",
        "\n",
        "[ LayerNorm over (stocks S, features D) ]\n",
        " → (B, S, T₁, D)\n",
        "\n",
        "\n",
        "[ StockAttention ]\n",
        " • learnable Wₛ ∈ ℝ^(S×S), bₛ ∈ ℝ^S  \n",
        " • softmax over stocks → aₛ scores (shape S)  \n",
        " • y = ∑_{j=1..S} x_{…j…} · aₛⱼ + current-stock embedding  \n",
        " → (B, S, T₁, D)\n",
        "\n",
        "\n",
        "[ reshape for RNN ]\n",
        " (B, S, T₁, D) → ((B·S), T₁, D)\n",
        "\n",
        "\n",
        "[ GRU ]\n",
        " • input_size = D, hidden_size = D  \n",
        " • num_layers, dropout…  \n",
        " → outputs ((B·S), T₁, D)\n",
        "\n",
        "\n",
        "[ reshape back ]\n",
        " ((B·S), T₁, D) → (B, S, T₁, D)\n",
        "\n",
        "\n",
        "[ TimeAttention ]\n",
        " • learnable wₜ ∈ ℝ^(T₁)  \n",
        " • softmax over time → aₜ scores  \n",
        " • compress via ∑_{t=1..T₁} x_{…t…}·aₜ  \n",
        " → (B, S, D)\n",
        "\n",
        "\n",
        "[ Linear → 1 ]\n",
        " • Dense(D → 1) + bias + learned scaling & shift  \n",
        " • exp(·)  \n",
        " → “vol” prediction per stock (shape (B, S))\n",
        "\n",
        "\n",
        "[ LOSS = RMSPE(vol, target) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RDJ92DA1bz4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install einops\n",
        "!pip install lightning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ixGEC9j_1bz5"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import re\n",
        "import os\n",
        "import einops\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from lightning.pytorch.loggers import CSVLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8PM9w3PSFNc",
        "outputId": "68dcf881-223b-4513-89fa-5cfb4bc4be1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/RBS DL 2025/PRO/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nENivPyq1bz6"
      },
      "outputs": [],
      "source": [
        "n_features = 21\n",
        "n_stocks = 112\n",
        "n_seconds = 600\n",
        "# if coarsen > 1, data will be aggregated per this number of seconds. used to reduce mem usage\n",
        "coarsen = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzm_gLXKIP4d"
      },
      "source": [
        "So the four dimensions are:\n",
        "\t1.\tBatch (N)\n",
        "\t2.\tStocks (112 in the competition)\n",
        "\t3.\tTime slices (600 seconds divided by your coarsen stride → e.g. 200)\n",
        "\t4.\tFeatures (21 raw+engineered columns: prices, sizes, log‐returns, spreads, etc.)\n",
        "\n",
        "That 4D structure comes out of our preprocessing in prepare_data, where we:\n",
        "\t•\tRead per-stock parquet files,\n",
        "\t•\tAlign everything on the same time_id×seconds_in_bucket grid,\n",
        "\t•\tEngineer the 21 features,\n",
        "\t•\tCoarsen and stack them into a big NumPy array or xarray with shape (T, S, secs, F),\n",
        "\n",
        "then wrap that into PyTorch datasets to yield mini-batches of shape (batch, S, secs, F)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h7l0wMFOHOWO"
      },
      "outputs": [],
      "source": [
        "# used later for plotting historical train/val results\n",
        "class MetricHistory(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.train_vol = []\n",
        "        self.valid_vol = []\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        m = trainer.callback_metrics\n",
        "        self.train_vol.append(m['train/vol_loss'].cpu().item())\n",
        "        self.valid_vol.append(m['valid/vol_loss'].cpu().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XQR4Rzfvc4HH"
      },
      "outputs": [],
      "source": [
        "def prepare_data(stock_id, stock_ind, set, time_ids, coarsen, norm, out):\n",
        "    #load book data\n",
        "    df_book = pd.read_parquet(f'{data_dir}/book_{set}.parquet/stock_id={stock_id}')\n",
        "    df_min_second = df_book.groupby('time_id').agg(min_second=('seconds_in_bucket', 'min'))\n",
        "    df_book = df_book.merge(df_min_second, left_on='time_id', right_index=True) \\\n",
        "        .eval('seconds_in_bucket = seconds_in_bucket - min_second') \\\n",
        "        .drop('min_second', axis=1)\n",
        "    # load trade data\n",
        "    df_trade = pd.read_parquet(f'{data_dir}/trade_{set}.parquet/stock_id={stock_id}') \\\n",
        "        .merge(df_min_second, left_on='time_id', right_index=True) \\\n",
        "        .eval('seconds_in_bucket = seconds_in_bucket - min_second') \\\n",
        "        .drop('min_second', axis=1)\n",
        "    # merge book + trade\n",
        "    df = pd.merge(df_book, df_trade, on=['time_id', 'seconds_in_bucket'], how='outer')\n",
        "    df['stock_id'] = stock_id\n",
        "    # set multi index\n",
        "    df = df.set_index(['stock_id', 'time_id', 'seconds_in_bucket'])\n",
        "    # pandas -> xarray\n",
        "    df = df.to_xarray().astype('float32')\n",
        "    # processing seconds col to make sure it works fine\n",
        "    df = df.reindex({'time_id': time_ids, 'seconds_in_bucket': np.arange(n_seconds)})\n",
        "    # forward fill imputation: if no new quote, old quote stays active\n",
        "    for name in ['bid_price1', 'bid_price2', 'ask_price1', 'ask_price2',\n",
        "         'bid_size1', 'bid_size2', 'ask_size1', 'ask_size2']:\n",
        "        df[name] = df[name].ffill('seconds_in_bucket')\n",
        "    # wap1/2\n",
        "    df['wap1'] = (df.bid_price1 * df.ask_size1 + df.ask_price1 * df.bid_size1) / (df.bid_size1 + df.ask_size1)\n",
        "    df['wap2'] = (df.bid_price2 * df.ask_size2 + df.ask_price2 * df.bid_size2) / (df.bid_size2 + df.ask_size2)\n",
        "    # log(wap1/2)\n",
        "    df['log_return1'] = np.log(df.wap1).diff('seconds_in_bucket')\n",
        "    df['log_return2'] = np.log(df.wap2).diff('seconds_in_bucket')\n",
        "    df['current_vol'] = (df.log_return1 ** 2).sum('seconds_in_bucket') ** 0.5\n",
        "    df['current_vol_2nd_half'] = (df.log_return1[..., 300:] ** 2).sum('seconds_in_bucket') ** 0.5\n",
        "    # downsmapling if coursen > 1\n",
        "    if coarsen > 1:\n",
        "        mean_features = ['ask_price1', 'ask_price2', 'bid_price1', 'bid_price2',  'ask_size1', 'ask_size2',\n",
        "               'bid_size1', 'bid_size2', 'price']\n",
        "        sum_features = ['size', 'order_count']\n",
        "\n",
        "        df = xr.merge((df[mean_features].coarsen({'seconds_in_bucket': coarsen}, coord_func='min').mean(),\n",
        "                       df[sum_features].coarsen({'seconds_in_bucket': coarsen}, coord_func='min').sum(),\n",
        "                       df[['current_vol', 'current_vol_2nd_half']]))\n",
        "        df['wap1'] = (df.bid_price1 * df.ask_size1 + df.ask_price1 * df.bid_size1) / (df.bid_size1 + df.ask_size1)\n",
        "        df['wap2'] = (df.bid_price2 * df.ask_size2 + df.ask_price2 * df.bid_size2) / (df.bid_size2 + df.ask_size2)\n",
        "        df['log_return1'] = np.log(df.wap1).diff('seconds_in_bucket')\n",
        "        df['log_return2'] = np.log(df.wap2).diff('seconds_in_bucket')\n",
        "    # ba spread\n",
        "    df['spread1'] = df.ask_price1 - df.bid_price1\n",
        "    # order book slope\n",
        "    df['spread2'] = df.ask_price2 - df.ask_price1\n",
        "    df['spread3'] = df.bid_price1 - df.bid_price2\n",
        "    df['total_volume'] = df.ask_size1 + df.ask_size2 + df.bid_size1 + df.bid_size2\n",
        "    df['volume_imbalance1'] = df.ask_size1 + df.ask_size2 - df.bid_size1 - df.bid_size2\n",
        "    df['volume_imbalance2'] = (df.ask_size1 + df.ask_size2 - df.bid_size1 - df.bid_size2) / df.total_volume\n",
        "    for name in ['bid_size1', 'bid_size2', 'ask_size1', 'ask_size2', 'size', 'order_count', 'total_volume']:\n",
        "        df[name] = np.log1p(df[name])\n",
        "        # df[name] = df[name].rank('seconds_in_bucket')\n",
        "    df['volume_imbalance1'] = np.sign(df['volume_imbalance1']) * np.log1p(abs(df['volume_imbalance1']))\n",
        "\n",
        "    df = df.fillna({'ask_price1': 1, 'ask_price2': 1, 'bid_price1': 1, 'bid_price2': 1,  'ask_size1': 0, 'ask_size2': 0,\n",
        "               'bid_size1': 0, 'bid_size2': 0, 'price': 1, 'size': 0, 'order_count': 0, 'wap1': 1, 'wap2': 1,\n",
        "               'log_return1': 0, 'log_return2': 0, 'spread1': 0, 'spread2': 0, 'spread3': 0, 'total_volume': 0,\n",
        "               'volume_imbalance1': 0, 'volume_imbalance2': 0, 'current_vol': 0, 'current_vol_2nd_half': 0})\n",
        "    features = ['ask_price1', 'ask_price2', 'bid_price1', 'bid_price2',  'ask_size1', 'ask_size2',\n",
        "               'bid_size1', 'bid_size2', 'price', 'size', 'order_count', 'wap1', 'wap2',\n",
        "               'log_return1', 'log_return2', 'spread1', 'spread2', 'spread3', 'total_volume',\n",
        "               'volume_imbalance1', 'volume_imbalance2']\n",
        "    extra = ['current_vol', 'current_vol_2nd_half']\n",
        "\n",
        "    if norm is not None:\n",
        "        mean = norm['mean'].sel(stock_id=stock_id)\n",
        "        std = norm['std'].sel(stock_id=stock_id)\n",
        "    else:\n",
        "        mean = df.mean(('time_id', 'seconds_in_bucket')).drop(['current_vol', 'current_vol_2nd_half'])\n",
        "        std = df.std(('time_id', 'seconds_in_bucket')).drop(['current_vol', 'current_vol_2nd_half'])\n",
        "\n",
        "    df.update((df - mean) / std)\n",
        "    df = df.astype('float32')\n",
        "\n",
        "    out[:, stock_ind] = einops.rearrange(df[features].to_array().values, 'f () t sec -> t sec f')\n",
        "    return df[extra], {'mean': mean, 'std': std}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D3Yq6DARdG0M"
      },
      "outputs": [],
      "source": [
        "class OptiverDataset(Dataset):\n",
        "    def __init__(self, features_data, extra_data, time_ids):\n",
        "        self.features_data = features_data\n",
        "        self.extra_data    = extra_data\n",
        "        self.time_ids      = time_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.time_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # find the time index\n",
        "        time_id = self.time_ids[i]\n",
        "        t_idx   = self.extra_data.indexes['time_id'].get_loc(time_id)\n",
        "\n",
        "        return {\n",
        "            'data': self.features_data[t_idx],                           # (n_stocks, n_secs, n_feat)\n",
        "            'target': self.extra_data['target'].values[t_idx],             # (n_stocks,)\n",
        "            'current_vol': self.extra_data['current_vol'].values[t_idx],        # (n_stocks,)\n",
        "            'current_vol_2nd_half': self.extra_data['current_vol_2nd_half'].values[t_idx],# (n_stocks,)\n",
        "            'time_id': time_id\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOuRz3GP4ccR"
      },
      "source": [
        "Time Attention Intuition:\n",
        "\n",
        "Maybe volatility spikes at the beginning of a window. Or maybe at the end of a window. Fixed pooling (mean, max) can't adapt to this. This TimeAttention layer learns to adapt dynamically based on data patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hh0XK_9zg_xV"
      },
      "outputs": [],
      "source": [
        "# not all seconds are equally informative — attention helps model prioritize\n",
        "class TimeAttention(nn.Module):\n",
        "    # learn which parts of the time window are important\n",
        "    def __init__(self, steps):\n",
        "        super().__init__()\n",
        "        self.steps = steps\n",
        "        self.weights = nn.Parameter(torch.zeros(steps))\n",
        "\n",
        "    # the forward pass - collpases time dimension in a weighted manner\n",
        "    def forward(self, x):\n",
        "        # x: (b, st, t, f)\n",
        "        attn = F.softmax(self.weights, 0)\n",
        "        x = torch.einsum('b s t f, t -> b s f', x, attn)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riH_2FSi4O9u"
      },
      "source": [
        "Stock Attention Intuition:\n",
        "Each stock might borrow information from other correlated stocks.\n",
        "\n",
        "Example:\n",
        "\n",
        "If stock_10 and stock_25 usually move together, attention will learn to pull information between them. Helps the model generalize better, especially when market events affect multiple stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vnckGz131W8x"
      },
      "outputs": [],
      "source": [
        "# You can experiment with other ideas for stock attention: maybe it could be\n",
        "# something like MultiHeadAttention module with keys and queries that depends on current input,\n",
        "# maybe it could be a linear combination of all stocks (full connected layer),maybe you can try sparse softmax\n",
        "\n",
        "class StockAttention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros((n_stocks, n_stocks)))\n",
        "        self.bias = nn.Parameter(torch.zeros(n_stocks))\n",
        "        self.fc_combine = nn.Linear(dim * 2, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, stock, time, feature)\n",
        "        # compute attention scores across assets\n",
        "        attn = F.softmax(self.weight + self.bias[None, :], dim = -1) # (st, st)\n",
        "        y = torch.einsum('b i ..., j i -> b j ...', x, attn)\n",
        "        x = torch.cat((x, y), -1)\n",
        "        x = self.fc_combine(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3xxWkKcS8OtR"
      },
      "outputs": [],
      "source": [
        "class OptiverModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mode='multi-stock',\n",
        "        dim=32,\n",
        "        conv1_kernel=3,\n",
        "        rnn_layers=2,\n",
        "        rnn_dropout=0.3,\n",
        "        n_features=21,\n",
        "        aux_loss_weight=1.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # embedding + conv layers\n",
        "        self.stock_emb = nn.Embedding(n_stocks, dim)\n",
        "        nn.init.normal_(self.stock_emb.weight, 0, 0.2)\n",
        "\n",
        "        self.conv1 = nn.Conv1d(n_features, dim, conv1_kernel, conv1_kernel)\n",
        "        self.conv2 = nn.Conv1d(dim, dim, 1, 1)\n",
        "        self.norm1 = nn.LayerNorm([n_stocks, dim])\n",
        "        self.norm2 = nn.LayerNorm([n_stocks, dim])\n",
        "\n",
        "        # RNN + attention modules\n",
        "        self.rnn = nn.GRU(dim, dim, rnn_layers, batch_first=True, dropout=rnn_dropout)\n",
        "        self.timesteps_attn = TimeAttention(600 // conv1_kernel // coarsen)\n",
        "        self.timesteps_attn2 = TimeAttention(300 // conv1_kernel // coarsen)\n",
        "        self.stock_attn = StockAttention(dim)\n",
        "\n",
        "        # output heads\n",
        "        self.fc_out1 = nn.Linear(dim, 1)\n",
        "        self.fc_out2 = nn.Linear(dim, 1)\n",
        "\n",
        "        # a pandas frame to collect per‐epoch metrics for plotting\n",
        "        self.history = pd.DataFrame()\n",
        "\n",
        "    def forward(self, x, stock_ind=None):\n",
        "        # x: (b, st, t, f)\n",
        "        b, st, t, f = x.shape\n",
        "\n",
        "        # conv block #1\n",
        "        x = einops.rearrange(x, 'b st t f -> (b st) f t')\n",
        "        x = self.conv1(x)\n",
        "        x = F.gelu(einops.rearrange(x, '(b st) f t -> b t st f', st=st))\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # conv block #2\n",
        "        x = einops.rearrange(x, 'b t st f -> (b st) f t')\n",
        "        x = self.conv2(x)\n",
        "        x = F.gelu(einops.rearrange(x, '(b st) f t -> b t st f', st=st))\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # stock‐level attention + add stock embedding\n",
        "        x = einops.rearrange(x, 'b t st f -> b st t f')\n",
        "        x = self.stock_attn(x)\n",
        "        x = x + self.stock_emb.weight[None, :, None, :]\n",
        "\n",
        "        # if single‐stock mode, select current‐stock slice here…\n",
        "        if self.hparams.mode == 'single-stock':\n",
        "            x = x[torch.arange(len(x)), stock_ind][:, None]\n",
        "\n",
        "        # RNN\n",
        "        x = einops.rearrange(x, 'b st t f -> (b st) t f')\n",
        "        x, _ = self.rnn(x)\n",
        "        x = einops.rearrange(x, '(b st) t f -> b st t f', st=st if self.hparams.mode=='multi-stock' else 1)\n",
        "\n",
        "        # time attention & output head #1 / full window\n",
        "        x1 = self.timesteps_attn(x)\n",
        "        x1 = torch.exp(self.fc_out1(x1) * 0.63393 - 5.762331)\n",
        "\n",
        "        # time attention #2 (second half) & output head #2\n",
        "        x2 = self.timesteps_attn2(x[:, :, :self.timesteps_attn2.steps, :])\n",
        "        x2 = torch.exp(self.fc_out2(x2) * 0.67473418 - 6.098946)\n",
        "\n",
        "        if self.hparams.mode == 'single-stock':\n",
        "            return {'vol': x1[:, 0, 0], 'vol2': x2[:, 0, 0]}\n",
        "        else:\n",
        "            return {'vol': x1[..., 0], 'vol2': x2[..., 0]}\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.common_step(batch, 'train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.common_step(batch, 'valid')\n",
        "\n",
        "    def common_step(self, batch, stage):\n",
        "        out = self(batch['data'], batch.get('stock_ind', None))\n",
        "        # primary target\n",
        "        mask1   = ~torch.isnan(batch['target'])\n",
        "        target1 = torch.where(mask1, batch['target'], torch.tensor(1.0, device=self.device))\n",
        "        # aux target\n",
        "        mask2   = batch['current_vol_2nd_half'] > 0\n",
        "        target2 = torch.where(mask2, batch['current_vol_2nd_half'], torch.tensor(1.0, device=self.device))\n",
        "\n",
        "        vol_loss  = (((out['vol']  - target1) / target1) ** 2)[mask1].mean().sqrt()\n",
        "        vol2_loss = (((out['vol2'] - target2) / target2) ** 2)[mask2].mean().sqrt()\n",
        "        loss = vol_loss + self.hparams.aux_loss_weight * vol2_loss\n",
        "\n",
        "        # log scalars\n",
        "        self.log(f'{stage}/loss',      loss,      on_epoch=True, prog_bar=True)\n",
        "        self.log(f'{stage}/vol_loss',  vol_loss,  on_epoch=True)\n",
        "        self.log(f'{stage}/vol2_loss', vol2_loss, on_epoch=True)\n",
        "        return {'loss': loss, 'vol': out['vol'], 'target': batch['target'], 'time_id': batch['time_id']}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        self._epoch_end(outputs, 'train')\n",
        "        # display live plot in colab/notebook\n",
        "        self.history_widget.clear_output(wait=True)\n",
        "        with self.history_widget:\n",
        "            ax = self.history[['train/rmspe','valid/rmspe']].plot(style=['-','--'])\n",
        "            ax.set_ylim(self.history.min().min(), self.history.quantile(0.95).max())\n",
        "            plt.show()\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self._epoch_end(outputs, 'valid')\n",
        "\n",
        "    def _epoch_end(self, outputs, stage):\n",
        "        # collate all batch‐outputs\n",
        "        vols     = torch.cat([o['vol']    for o in outputs])\n",
        "        targets  = torch.cat([o['target'] for o in outputs])\n",
        "        mask     = ~torch.isnan(targets)\n",
        "        rmspe    = (((vols[mask]-targets[mask]) / targets[mask])**2).mean().sqrt()\n",
        "        # save into history\n",
        "        self.history.loc[self.current_epoch, f'{stage}/rmspe'] = rmspe.item()\n",
        "        # also log it so lightning’s callback_metrics picks it up\n",
        "        self.log(f'{stage}/rmspe', rmspe, prog_bar=True, on_epoch=True)\n",
        "\n",
        "    def on_fit_start(self):\n",
        "        # create our live‐plot widget\n",
        "        self.history_widget = widgets.Output()\n",
        "        display(self.history_widget)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters(), lr=1e-3)\n",
        "        scheduler = ExponentialLR(optimizer, gamma=0.93)\n",
        "        return [optimizer], [{'scheduler': scheduler, 'interval': 'epoch'}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pZ5t_0Ux1bz7"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(f'{data_dir}/train.csv')\n",
        "train_data = np.memmap('/content/drive/MyDrive/Colab Notebooks/RBS DL 2025/PRO/train.npy', 'float16', 'w+',\n",
        "                       shape=(df_train.time_id.nunique(), n_stocks, n_seconds // coarsen, n_features))\n",
        "\n",
        "res = Parallel(n_jobs=4, verbose=51)(\n",
        "    delayed(prepare_data)(stock_id, stock_ind, 'train', df_train.time_id.unique(), coarsen, None, train_data)\n",
        "    for stock_ind, stock_id in enumerate(df_train.stock_id.unique())\n",
        ")\n",
        "\n",
        "train_extra = xr.concat([x[0] for x in res], 'stock_id')\n",
        "train_extra['target'] = df_train.set_index(['time_id', 'stock_id']).to_xarray()['target'].astype('float32')\n",
        "train_extra = train_extra.transpose('time_id', 'stock_id')\n",
        "train_norm = {\n",
        "    'mean': xr.concat([x[1]['mean'] for x in res], 'stock_id'),\n",
        "    'std': xr.concat([x[1]['std'] for x in res], 'stock_id')\n",
        "}\n",
        "\n",
        "# if you data fits in memory, you can load it entirely from disk, otherwise\n",
        "# training in single-stock mode could be very slow, though it can be OK for multi-stock mode\n",
        "train_data = np.array(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5bMAxU_e1bz8"
      },
      "outputs": [],
      "source": [
        "# 1) split your time_ids into train/valid\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "time_ids = train_extra.indexes['time_id'].values\n",
        "train_idx, valid_idx = next(cv.split(time_ids))\n",
        "\n",
        "# 2) wrap each in our simplified dataset\n",
        "train_ds = OptiverDataset(\n",
        "    features_data = train_data,\n",
        "    extra_data    = train_extra,\n",
        "    time_ids      = time_ids[train_idx],\n",
        ")\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "valid_ds = OptiverDataset(\n",
        "    features_data = train_data,\n",
        "    extra_data    = train_extra,\n",
        "    time_ids      = time_ids[valid_idx],\n",
        ")\n",
        "\n",
        "valid_dl = DataLoader(\n",
        "    valid_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FTbJF0W71bz8"
      },
      "outputs": [],
      "source": [
        "model = OptiverModel(dim=32, conv1_kernel=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9nZlyF86AwbZ"
      },
      "outputs": [],
      "source": [
        "csv_logger = CSVLogger(\n",
        "    save_dir='logs',\n",
        "    name='optiver'\n",
        ")\n",
        "\n",
        "# monitor whatever metric you care about; here I pick valid/vol_loss\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    monitor='valid/vol_loss',   # our RMSPE on vol\n",
        "    mode='min',\n",
        "    save_top_k=1,\n",
        "    filename='epoch{epoch:02d}-vol{valid/vol_loss:.4f}'\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"valid/vol_loss\",   # the metric to watch\n",
        "    patience=5,                 # how many epochs to wait for improvement\n",
        "    mode=\"min\",                 # we want to minimize vol_loss\n",
        "    strict=True,                # error if monitored metric isn’t found\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    devices=1,\n",
        "    accelerator='gpu',\n",
        "    precision=16,\n",
        "    max_epochs=10,\n",
        "    callbacks=[checkpoint_cb, early_stop],\n",
        "    logger=csv_logger\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5TzGl2i1bz-"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTlYSzRaDlqG"
      },
      "outputs": [],
      "source": [
        "# (Optional) see exactly what keys were logged\n",
        "print(\"All logged metrics:\", trainer.callback_metrics)\n",
        "\n",
        "# Now print whichever metrics you care about—e.g. the primary volatility loss:\n",
        "print(f\"Final train vol_loss: {trainer.callback_metrics['train/vol_loss']:.5f}\")\n",
        "print(f\"Final valid vol_loss: {trainer.callback_metrics['valid/vol_loss']:.5f}\")\n",
        "\n",
        "# If you also want the overall loss:\n",
        "print(f\"Final train loss:     {trainer.callback_metrics['train/loss']:.5f}\")\n",
        "print(f\"Final valid loss:     {trainer.callback_metrics['valid/loss']:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgaUtABE0NlI"
      },
      "outputs": [],
      "source": [
        "# lightning has already saved the best‐ever checkpoint under `checkpoint_cb.best_model_path`\n",
        "best_score = checkpoint_cb.best_model_score\n",
        "best_path  = checkpoint_cb.best_model_path\n",
        "\n",
        "print(f\"✔️  Best valid/vol_loss = {best_score:.5f}\")\n",
        "print(f\"   (checkpoint saved to: {best_path})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaC2u5Lb6YNg"
      },
      "outputs": [],
      "source": [
        "best_path  = checkpoint_cb.best_model_path\n",
        "best_score = checkpoint_cb.best_model_score  # torch.Tensor(0.2118)\n",
        "\n",
        "# extract the epoch number from the filename\n",
        "m = re.search(r\"epoch(\\d+)-\", best_path)\n",
        "best_epoch = int(m.group(1)) if m else None\n",
        "\n",
        "print(f\"✔️  Best epoch by valid/vol_loss = {best_epoch}\")\n",
        "print(f\"    Best valid/vol_loss (RMSPE)    = {best_score:.5f}\")\n",
        "print(f\"    Checkpoint path: {best_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ICw_lSV1bz_"
      },
      "outputs": [],
      "source": [
        "pd.Series(F.softmax(model.timesteps_attn.weights, 0).detach()).plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdtAoy3M-WYY"
      },
      "outputs": [],
      "source": [
        "# — 1) Load the raw metrics.csv that Lightning’s CSVLogger wrote —\n",
        "#    You should already have defined:\n",
        "#      csv_logger = CSVLogger(...)\n",
        "#      trainer = pl.Trainer(..., logger=csv_logger)\n",
        "#    so that csv_logger.log_dir points to the folder containing metrics.csv.\n",
        "\n",
        "metrics_path = os.path.join(csv_logger.log_dir, \"metrics.csv\")\n",
        "metrics = pd.read_csv(metrics_path)\n",
        "\n",
        "# — 2) Quick sanity check — which columns do you actually have?\n",
        "print(\">>> Logged columns:\", metrics.columns.tolist())\n",
        "print(metrics.head())\n",
        "\n",
        "# — 3) One row per epoch — group & take the mean of the two per-epoch entries —\n",
        "#    (Lightning logs train & val in separate rows, so we average them here)\n",
        "epoch_metrics = (\n",
        "    metrics\n",
        "    .groupby(\"epoch\")[[\"train/vol_loss\", \"valid/vol_loss\"]]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# — 4) Plot it —\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(\n",
        "    epoch_metrics[\"epoch\"],\n",
        "    epoch_metrics[\"train/vol_loss\"],\n",
        "    label=\"train vol_loss (RMSPE)\",\n",
        ")\n",
        "plt.plot(\n",
        "    epoch_metrics[\"epoch\"],\n",
        "    epoch_metrics[\"valid/vol_loss\"],\n",
        "    \"--\",\n",
        "    label=\"valid vol_loss (RMSPE)\",\n",
        ")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Vol-loss (RMSPE)\")\n",
        "plt.title(\"Train vs. Valid RMSPE over Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwnDnw481b0A"
      },
      "outputs": [],
      "source": [
        "!rm \"/content/drive/MyDrive/Colab Notebooks/RBS DL 2025/PRO/train.npy\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 2344753,
          "sourceId": 27233,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30140,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}