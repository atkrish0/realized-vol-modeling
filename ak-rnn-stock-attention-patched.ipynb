{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IysNA2h61bz2"
   },
   "source": [
    "I've made this notebook to demonstrate my solution, you can read detailed explanation [here](https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/279170). It doesn't include full CV and inference, instead I tried to make it short and include only key elements: preprocessing and NN architecture. I did a lot of refactoring to simplify my code and to fit kaggle RAM limitations, it might affect performance a bit, but hopefully not so much.\n",
    "\n",
    "One thing that I changed is data preprocessing using xarray instead of pandas - I really liked how well it fit multidimensional nature of the data. It speed up many group by operations by replacing them with aggregations over array dimensions.\n",
    "\n",
    "Here I also include 2 modes of training: *single-stock* and *multi-stock*. *multi-stock* training works really fast and you can get decent results after 20 minutes. In this mode train batch includes targets for all stocks in time_id and you have only 3830 training samples. In *single-stock* mode input is still the same and includes data from all stocks, but it also has single stock_id as input and only single stock target for it is predicted. This way batch contains much more diverse (stock_id, time_id) pairs, and I believe this diversity is important to get better score. During the competition I used *single-stock* training, despite much longer training times. Usually my public score was better than my validation score by 0.005-0.006. My best single models without using nearest neigbours scored ~0.199-0.200 on public while validation score was ~0.206. In *multi-stock* mode I can achieve currently only ~0.210-0.211 score, though I haven't tried to wait until training end in *single-stock* mode after refactoring, but it should be better.\n",
    "\n",
    "Anyway this just a baseline, and there are a lot of things to experiment with: stock attention placement (before/after RNN and internal implementation, feature normalization (you should probably do something with volumes as the way I did it didn't work well on private test), network dimensions, batch size, lr, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-15T22:20:00.851113Z",
     "iopub.status.busy": "2025-04-15T22:20:00.850814Z",
     "iopub.status.idle": "2025-04-15T22:20:08.462014Z",
     "shell.execute_reply": "2025-04-15T22:20:08.461032Z",
     "shell.execute_reply.started": "2025-04-15T22:20:00.851024Z"
    },
    "executionInfo": {
     "elapsed": 2915,
     "status": "ok",
     "timestamp": 1744827332034,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "1RDJ92DA1bz4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T22:20:12.392899Z",
     "iopub.status.busy": "2025-04-15T22:20:12.392162Z",
     "iopub.status.idle": "2025-04-15T22:20:18.450201Z",
     "shell.execute_reply": "2025-04-15T22:20:18.449533Z",
     "shell.execute_reply.started": "2025-04-15T22:20:12.392861Z"
    },
    "executionInfo": {
     "elapsed": 17258,
     "status": "error",
     "timestamp": 1744827349303,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "ixGEC9j_1bz5",
    "outputId": "0ccf96c5-7e24-4e24-a3f3-8984f36f8160"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-34a66ce7f47d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxarray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import einops\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1744827349325,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "w8PM9w3PSFNc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data_dir = '/content/drive/MyDrive/Colab Notebooks/RBS DL 2025/PRO/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T22:20:31.692854Z",
     "iopub.status.busy": "2025-04-15T22:20:31.692157Z",
     "iopub.status.idle": "2025-04-15T22:20:31.696431Z",
     "shell.execute_reply": "2025-04-15T22:20:31.695660Z",
     "shell.execute_reply.started": "2025-04-15T22:20:31.692822Z"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "aborted",
     "timestamp": 1744827349326,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "nENivPyq1bz6"
   },
   "outputs": [],
   "source": [
    "n_features = 21\n",
    "n_stocks = 112\n",
    "n_seconds = 600\n",
    "# if coarsen > 1, data will be aggregated per this number of seconds,\n",
    "# use this to reduce memory usage, though during competition I trained on full data,\n",
    "# so I'm not sure how it can affect model performance\n",
    "coarsen = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T22:20:55.494823Z",
     "iopub.status.busy": "2025-04-15T22:20:55.494582Z",
     "iopub.status.idle": "2025-04-15T22:20:55.542312Z",
     "shell.execute_reply": "2025-04-15T22:20:55.541518Z",
     "shell.execute_reply.started": "2025-04-15T22:20:55.494800Z"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "aborted",
     "timestamp": 1744827349346,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "xF1iKEUv1bz6"
   },
   "outputs": [],
   "source": [
    "def prepare_data(stock_id, stock_ind, set, time_ids, coarsen, norm, out):\n",
    "    df_book = pd.read_parquet(f'{data_dir}/book_{set}.parquet/stock_id={stock_id}')\n",
    "    df_min_second = df_book.groupby('time_id').agg(min_second=('seconds_in_bucket', 'min'))\n",
    "    df_book = df_book.merge(df_min_second, left_on='time_id', right_index=True) \\\n",
    "        .eval('seconds_in_bucket = seconds_in_bucket - min_second') \\\n",
    "        .drop('min_second', axis=1)\n",
    "    df_trade = pd.read_parquet(f'{data_dir}/trade_{set}.parquet/stock_id={stock_id}') \\\n",
    "        .merge(df_min_second, left_on='time_id', right_index=True) \\\n",
    "        .eval('seconds_in_bucket = seconds_in_bucket - min_second') \\\n",
    "        .drop('min_second', axis=1)\n",
    "    df = pd.merge(df_book, df_trade, on=['time_id', 'seconds_in_bucket'], how='outer')\n",
    "    df['stock_id'] = stock_id\n",
    "    df = df.set_index(['stock_id', 'time_id', 'seconds_in_bucket'])\n",
    "    df = df.to_xarray().astype('float32')\n",
    "    df = df.reindex({'time_id': time_ids, 'seconds_in_bucket': np.arange(n_seconds)})\n",
    "    for name in ['bid_price1', 'bid_price2', 'ask_price1', 'ask_price2',\n",
    "         'bid_size1', 'bid_size2', 'ask_size1', 'ask_size2']:\n",
    "        df[name] = df[name].ffill('seconds_in_bucket')\n",
    "    df['wap1'] = (df.bid_price1 * df.ask_size1 + df.ask_price1 * df.bid_size1) / (df.bid_size1 + df.ask_size1)\n",
    "    df['wap2'] = (df.bid_price2 * df.ask_size2 + df.ask_price2 * df.bid_size2) / (df.bid_size2 + df.ask_size2)\n",
    "    df['log_return1'] = np.log(df.wap1).diff('seconds_in_bucket')\n",
    "    df['log_return2'] = np.log(df.wap2).diff('seconds_in_bucket')\n",
    "    df['current_vol'] = (df.log_return1 ** 2).sum('seconds_in_bucket') ** 0.5\n",
    "    df['current_vol_2nd_half'] = (df.log_return1[..., 300:] ** 2).sum('seconds_in_bucket') ** 0.5\n",
    "    if coarsen > 1:\n",
    "        mean_features = ['ask_price1', 'ask_price2', 'bid_price1', 'bid_price2',  'ask_size1', 'ask_size2',\n",
    "               'bid_size1', 'bid_size2', 'price']\n",
    "        sum_features = ['size', 'order_count']\n",
    "\n",
    "        df = xr.merge((df[mean_features].coarsen({'seconds_in_bucket': coarsen}, coord_func='min').mean(),\n",
    "                       df[sum_features].coarsen({'seconds_in_bucket': coarsen}, coord_func='min').sum(),\n",
    "                       df[['current_vol', 'current_vol_2nd_half']]))\n",
    "        df['wap1'] = (df.bid_price1 * df.ask_size1 + df.ask_price1 * df.bid_size1) / (df.bid_size1 + df.ask_size1)\n",
    "        df['wap2'] = (df.bid_price2 * df.ask_size2 + df.ask_price2 * df.bid_size2) / (df.bid_size2 + df.ask_size2)\n",
    "        df['log_return1'] = np.log(df.wap1).diff('seconds_in_bucket')\n",
    "        df['log_return2'] = np.log(df.wap2).diff('seconds_in_bucket')\n",
    "\n",
    "    df['spread1'] = df.ask_price1 - df.bid_price1\n",
    "    df['spread2'] = df.ask_price2 - df.ask_price1\n",
    "    df['spread3'] = df.bid_price1 - df.bid_price2\n",
    "    df['total_volume'] = df.ask_size1 + df.ask_size2 + df.bid_size1 + df.bid_size2\n",
    "    df['volume_imbalance1'] = df.ask_size1 + df.ask_size2 - df.bid_size1 - df.bid_size2\n",
    "    df['volume_imbalance2'] = (df.ask_size1 + df.ask_size2 - df.bid_size1 - df.bid_size2) / df.total_volume\n",
    "    for name in ['bid_size1', 'bid_size2', 'ask_size1', 'ask_size2', 'size', 'order_count', 'total_volume']:\n",
    "        df[name] = np.log1p(df[name])\n",
    "#         df[name] = df[name].rank('seconds_in_bucket')\n",
    "    df['volume_imbalance1'] = np.sign(df['volume_imbalance1']) * np.log1p(abs(df['volume_imbalance1']))\n",
    "\n",
    "    df = df.fillna({'ask_price1': 1, 'ask_price2': 1, 'bid_price1': 1, 'bid_price2': 1,  'ask_size1': 0, 'ask_size2': 0,\n",
    "               'bid_size1': 0, 'bid_size2': 0, 'price': 1, 'size': 0, 'order_count': 0, 'wap1': 1, 'wap2': 1,\n",
    "               'log_return1': 0, 'log_return2': 0, 'spread1': 0, 'spread2': 0, 'spread3': 0, 'total_volume': 0,\n",
    "               'volume_imbalance1': 0, 'volume_imbalance2': 0, 'current_vol': 0, 'current_vol_2nd_half': 0})\n",
    "    features = ['ask_price1', 'ask_price2', 'bid_price1', 'bid_price2',  'ask_size1', 'ask_size2',\n",
    "               'bid_size1', 'bid_size2', 'price', 'size', 'order_count', 'wap1', 'wap2',\n",
    "               'log_return1', 'log_return2', 'spread1', 'spread2', 'spread3', 'total_volume',\n",
    "               'volume_imbalance1', 'volume_imbalance2']\n",
    "    extra = ['current_vol', 'current_vol_2nd_half']\n",
    "\n",
    "    if norm is not None:\n",
    "        mean = norm['mean'].sel(stock_id=stock_id)\n",
    "        std = norm['std'].sel(stock_id=stock_id)\n",
    "    else:\n",
    "        mean = df.mean(('time_id', 'seconds_in_bucket')).drop(['current_vol', 'current_vol_2nd_half'])\n",
    "        std = df.std(('time_id', 'seconds_in_bucket')).drop(['current_vol', 'current_vol_2nd_half'])\n",
    "\n",
    "    df.update((df - mean) / std)\n",
    "    df = df.astype('float32')\n",
    "\n",
    "    out[:, stock_ind] = einops.rearrange(df[features].to_array().values, 'f () t sec -> t sec f')\n",
    "    return df[extra], {'mean': mean, 'std': std}\n",
    "\n",
    "class OptiverDataset(Dataset):\n",
    "    def __init__(self, features_data, extra_data, mode, time_ids):\n",
    "        self.features_data = features_data\n",
    "        self.extra_data = extra_data\n",
    "        self.time_ids = time_ids\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'single-stock':\n",
    "            return len(self.time_ids) * n_stocks\n",
    "        elif self.mode == 'multi-stock':\n",
    "            return len(self.time_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.mode == 'single-stock':\n",
    "            time_id = self.time_ids[i // n_stocks]\n",
    "            time_ind = self.extra_data.indexes['time_id'].get_loc(time_id)\n",
    "            stock_ind = i % n_stocks\n",
    "            stock_id = self.extra_data.indexes['stock_id'][stock_ind]\n",
    "            return {\n",
    "                'data': self.features_data[time_ind], # (112, 600, 21)\n",
    "                'target': self.extra_data['target'].values[time_ind, stock_ind],  # (1,)\n",
    "                'current_vol': self.extra_data['current_vol'].values[time_ind, stock_ind],  # (1,)\n",
    "                'current_vol_2nd_half': self.extra_data['current_vol_2nd_half'].values[time_ind, stock_ind],  # (1,)\n",
    "                'time_id': time_id,\n",
    "                'stock_id': stock_id,\n",
    "                'stock_ind': stock_ind\n",
    "            }\n",
    "        elif self.mode == 'multi-stock':\n",
    "            time_id = self.time_ids[i]\n",
    "            time_ind = self.extra_data.indexes['time_id'].get_loc(time_id)\n",
    "            return {\n",
    "                'data': self.features_data[time_ind], # (112, 600, 21)\n",
    "                'target': self.extra_data['target'].values[time_ind],  # (112,)\n",
    "                'current_vol': self.extra_data['current_vol'].values[time_ind],  # (112,)\n",
    "                'current_vol_2nd_half': self.extra_data['current_vol_2nd_half'].values[time_ind],  # (112,)\n",
    "                'time_id': time_id,\n",
    "            }\n",
    "\n",
    "class TimeAttention(nn.Module):\n",
    "    def __init__(self, steps):\n",
    "        super().__init__()\n",
    "        self.steps = steps\n",
    "        self.weights = nn.Parameter(torch.zeros(steps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (b, st, t, f)\n",
    "        attn = F.softmax(self.weights, 0)\n",
    "        x = torch.einsum('b s t f, t -> b s f', x, attn)\n",
    "        return x\n",
    "\n",
    "# You can experiment with other ideas for stock attention: maybe it could be\n",
    "# something like MultiHeadAttention module with keys and queries that depends on current input,\n",
    "# maybe it could be a linear combination of all stocks (full connected layer),\n",
    "# maybe you can try sparse softmax\n",
    "class StockAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros((n_stocks, n_stocks)))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_stocks))\n",
    "        self.fc_combine = nn.Linear(dim * 2, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (b, st, t, f)\n",
    "        attn = F.softmax(self.weight + self.bias[None, :], dim=-1) # (st, st)\n",
    "        y = torch.einsum('b i ..., j i -> b j ...', x, attn)\n",
    "        x = torch.cat((x, y), -1)\n",
    "        x = self.fc_combine(x)\n",
    "        return x\n",
    "\n",
    "class OptiverModel(pl.LightningModule):\n",
    "    def __init__(self, mode='multi-stock', dim=32, conv1_kernel=3, rnn_layers=2, rnn_dropout=0.3,\n",
    "                 n_features=21, aux_loss_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.stock_emb = nn.Embedding(n_stocks, dim)\n",
    "        self.stock_emb.weight.data.normal_(0, 0.2)\n",
    "        self.conv1 = nn.Conv1d(n_features, dim, conv1_kernel, conv1_kernel)\n",
    "        self.conv2 = nn.Conv1d(dim, dim, 1, 1)\n",
    "        self.norm1 = nn.LayerNorm([n_stocks, dim])\n",
    "        self.norm2 = nn.LayerNorm([n_stocks, dim])\n",
    "        self.rnn = nn.GRU(dim, dim, rnn_layers, batch_first=True, dropout=rnn_dropout)\n",
    "        self.timesteps_attn = TimeAttention(600 // conv1_kernel // coarsen)\n",
    "        self.timesteps_attn2 = TimeAttention(300 // conv1_kernel // coarsen)\n",
    "        self.stock_attn = StockAttention(dim)\n",
    "        self.fc_out1 = nn.Linear(dim, 1)\n",
    "        self.fc_out2 = nn.Linear(dim, 1)\n",
    "        self.history = pd.DataFrame()\n",
    "\n",
    "    def forward(self, x, stock_ind=None):\n",
    "        # x: (b, st, t, f)\n",
    "        x = einops.rearrange(x, 'b st t f -> (b st) f t')\n",
    "        x = self.conv1(x)\n",
    "        x = einops.rearrange(x, '(b st) f t -> b t st f', st=n_stocks)\n",
    "        x = F.gelu(x)\n",
    "        x = self.norm1(x)\n",
    "        x = einops.rearrange(x, 'b t st f -> (b st) f t')\n",
    "        x = self.conv2(x)\n",
    "        x = einops.rearrange(x, '(b st) f t -> b t st f', st=n_stocks)\n",
    "        x = F.gelu(x)\n",
    "        x = self.norm2(x)\n",
    "        x = einops.rearrange(x, 'b t st f -> b st t f')\n",
    "        x = self.stock_attn(x)\n",
    "        x = x + self.stock_emb.weight[None, :, None, :]\n",
    "        if self.hparams.mode == 'single-stock':\n",
    "            x = x[torch.arange(len(x)), stock_ind][:, None]\n",
    "        x = einops.rearrange(x, 'b st t f -> (b st) t f')\n",
    "        x = self.rnn(x)[0]\n",
    "        x = einops.rearrange(x, '(b st) t f -> b st t f', st=n_stocks if self.hparams.mode == 'multi-stock' else 1)\n",
    "        x1 = self.timesteps_attn(x)\n",
    "        x2 = self.timesteps_attn2(x[:, :, :self.timesteps_attn2.steps, :])\n",
    "        x1 = self.fc_out1(x1)\n",
    "        x2 = self.fc_out2(x2)\n",
    "        x1 = x1 * 0.63393 - 5.762331\n",
    "        x2 = x2 * 0.67473418 - 6.098946\n",
    "        x1 = torch.exp(x1)\n",
    "        x2 = torch.exp(x2)\n",
    "        if self.hparams.mode == 'single-stock':\n",
    "            return {\n",
    "                'vol': x1[:, 0, 0], # (b,)\n",
    "                'vol2': x2[:, 0, 0] # (b,)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'vol': x1[..., 0], # (b, st)\n",
    "                'vol2': x2[..., 0] # (b, st)\n",
    "            }\n",
    "\n",
    "    def training_step(self, batch, batch_ind):\n",
    "        out = self.common_step(batch, 'train')\n",
    "        return out\n",
    "\n",
    "    def validation_step(self, batch, batch_ind):\n",
    "        return self.common_step(batch, 'valid')\n",
    "\n",
    "    def common_step(self, batch, stage):\n",
    "        out = self(batch['data'], batch['stock_ind'] if self.hparams.mode == 'single-stock' else None)\n",
    "        mask1 = ~torch.isnan(batch['target'])\n",
    "        target1 = torch.where(mask1, batch['target'], torch.tensor(1.0, device=self.device))\n",
    "        mask2 = batch['current_vol_2nd_half'] > 0\n",
    "        target2 = torch.where(mask2, batch['current_vol_2nd_half'], torch.tensor(1.0, device=self.device))\n",
    "        vol_loss = (((out['vol'] - target1) / target1) ** 2)[mask1].mean() ** 0.5\n",
    "        vol2_loss = (((out['vol2'] - target2) / target2) ** 2)[mask2].mean() ** 0.5\n",
    "        loss = vol_loss + self.hparams.aux_loss_weight * vol2_loss\n",
    "        self.log(f'{stage}/loss', loss.item(), on_step=False, on_epoch=True)\n",
    "        self.log(f'{stage}/vol_loss', vol_loss.item(), on_step=False, on_epoch=True)\n",
    "        self.log(f'{stage}/vol2_loss', vol2_loss.item(), on_step=False, on_epoch=True)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'target': batch['target'],\n",
    "            'vol': out['vol'].detach(),\n",
    "            'time_id': batch['time_id']\n",
    "        }\n",
    "\n",
    "    def common_epoch_end(self, outs, stage):\n",
    "        target = torch.cat([x['target'] for x in outs])\n",
    "        vol = torch.cat([x['vol'] for x in outs])\n",
    "        time_ids = torch.cat([x['time_id'] for x in outs])\n",
    "        mask = ~torch.isnan(target)\n",
    "        target = torch.where(mask, target, torch.tensor(1.0, device=self.device))\n",
    "        rmspe = (((vol - target) / target) ** 2)[mask].mean() ** 0.5\n",
    "        self.log(f'{stage}/rmspe', rmspe, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.history.loc[self.trainer.current_epoch, f'{stage}/rmspe'] = rmspe.item()\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        self.common_epoch_end(outs, 'train')\n",
    "        self.history_widget.clear_output(wait=True)\n",
    "        with self.history_widget:\n",
    "            ylim = [self.history.min().min(), self.history.quantile(0.95).max()]\n",
    "            ylim[0] -= (ylim[1] - ylim[0]) * 0.05\n",
    "            self.history.plot(color=['C1', 'C0'], style=['--', '-'], ylim=ylim)\n",
    "            plt.show()\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.common_epoch_end(outs, 'valid')\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        self.history_widget = widgets.Output()\n",
    "        display(self.history_widget)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = Adam(self.parameters(), lr=0.001)\n",
    "#         opt = Adam(self.parameters(), lr=0.0005) # single-stock\n",
    "        sched = {\n",
    "            'scheduler': ExponentialLR(opt, 0.93),\n",
    "#             'scheduler': ExponentialLR(opt, 0.9), #  single-stock\n",
    "            'interval': 'epoch'\n",
    "        }\n",
    "        return [opt], [sched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T22:58:10.573632Z",
     "iopub.status.busy": "2025-04-15T22:58:10.573017Z",
     "iopub.status.idle": "2025-04-15T23:00:10.620863Z",
     "shell.execute_reply": "2025-04-15T23:00:10.620216Z",
     "shell.execute_reply.started": "2025-04-15T22:58:10.573597Z"
    },
    "executionInfo": {
     "elapsed": 20254,
     "status": "aborted",
     "timestamp": 1744827349347,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "pZ5t_0Ux1bz7"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f'{data_dir}/train.csv')\n",
    "train_data = np.zeros(,\n",
    "                       shape=(df_train.time_id.nunique(), n_stocks, n_seconds // coarsen, n_features))\n",
    "\n",
    "res = Parallel(n_jobs=4, verbose=51)(\n",
    "    delayed(prepare_data)(stock_id, stock_ind, 'train', df_train.time_id.unique(), coarsen, None, train_data)\n",
    "    for stock_ind, stock_id in enumerate(df_train.stock_id.unique())\n",
    ")\n",
    "\n",
    "train_extra = xr.concat([x[0] for x in res], 'stock_id')\n",
    "train_extra['target'] = df_train.set_index(['time_id', 'stock_id']).to_xarray()['target'].astype('float32')\n",
    "train_extra = train_extra.transpose('time_id', 'stock_id')\n",
    "train_norm = {\n",
    "    'mean': xr.concat([x[1]['mean'] for x in res], 'stock_id'),\n",
    "    'std': xr.concat([x[1]['std'] for x in res], 'stock_id')\n",
    "}\n",
    "\n",
    "# if you data fits in memory, you can load it entirely from disk, otherwise\n",
    "# training in single-stock mode could be very slow, though it can be OK for multi-stock mode\n",
    "train_data = np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:01:31.524950Z",
     "iopub.status.busy": "2025-04-15T23:01:31.524390Z",
     "iopub.status.idle": "2025-04-15T23:01:31.533220Z",
     "shell.execute_reply": "2025-04-15T23:01:31.532478Z",
     "shell.execute_reply.started": "2025-04-15T23:01:31.524915Z"
    },
    "executionInfo": {
     "elapsed": 20255,
     "status": "aborted",
     "timestamp": 1744827349348,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "5bMAxU_e1bz8"
   },
   "outputs": [],
   "source": [
    "cv = KFold(5, shuffle=True, random_state=1)\n",
    "time_ids = train_extra.indexes['time_id'].values\n",
    "train_time_ids, val_time_ids = next(cv.split(time_ids))\n",
    "\n",
    "# multi-stock training (fast)\n",
    "train_ds = OptiverDataset(train_data, train_extra, 'multi-stock', time_ids[train_time_ids])\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "val_ds = OptiverDataset(train_data, train_extra, 'multi-stock', time_ids[val_time_ids])\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "# single-stock training (slow)\n",
    "# train_ds = OptiverDataset(train_data, train_extra, 'single-stock', time_ids[train_time_ids])\n",
    "# train_dl = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "# val_ds = OptiverDataset(train_data, train_extra, 'single-stock', time_ids[val_time_ids])\n",
    "# val_dl = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:01:56.636733Z",
     "iopub.status.busy": "2025-04-15T23:01:56.636474Z",
     "iopub.status.idle": "2025-04-15T23:01:56.679045Z",
     "shell.execute_reply": "2025-04-15T23:01:56.678358Z",
     "shell.execute_reply.started": "2025-04-15T23:01:56.636706Z"
    },
    "executionInfo": {
     "elapsed": 20253,
     "status": "aborted",
     "timestamp": 1744827349348,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "FTbJF0W71bz8"
   },
   "outputs": [],
   "source": [
    "model = OptiverModel(mode='multi-stock', dim=32, conv1_kernel=1, aux_loss_weight=1) # multi-stock\n",
    "# model = OptiverModel(mode='single-stock', conv1_kernel=1, aux_loss_weight=0) # single-stock\n",
    "model.summarize(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:02:01.584626Z",
     "iopub.status.busy": "2025-04-15T23:02:01.583926Z",
     "iopub.status.idle": "2025-04-15T23:02:01.652422Z",
     "shell.execute_reply": "2025-04-15T23:02:01.651760Z",
     "shell.execute_reply.started": "2025-04-15T23:02:01.584590Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1744827349365,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "YkErma7h1bz9"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, precision=16, max_epochs=10) # multi-stock\n",
    "# trainer = pl.Trainer(gpus=1, precision=16, limit_train_batches=2500, max_epochs=25) # single-stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:02:04.858321Z",
     "iopub.status.busy": "2025-04-15T23:02:04.857675Z",
     "iopub.status.idle": "2025-04-15T23:28:46.468892Z",
     "shell.execute_reply": "2025-04-15T23:28:46.468120Z",
     "shell.execute_reply.started": "2025-04-15T23:02:04.858287Z"
    },
    "executionInfo": {
     "elapsed": 20270,
     "status": "aborted",
     "timestamp": 1744827349366,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "o5TzGl2i1bz-"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:38:39.817911Z",
     "iopub.status.busy": "2025-04-15T23:38:39.817143Z",
     "iopub.status.idle": "2025-04-15T23:38:39.823936Z",
     "shell.execute_reply": "2025-04-15T23:38:39.823187Z",
     "shell.execute_reply.started": "2025-04-15T23:38:39.817858Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1744827349387,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "FRwgAWXZ1bz_"
   },
   "outputs": [],
   "source": [
    "print(f'Best epoch {model.history[\"valid/rmspe\"].argmin()}: {model.history[\"valid/rmspe\"].min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:38:47.151192Z",
     "iopub.status.busy": "2025-04-15T23:38:47.150546Z",
     "iopub.status.idle": "2025-04-15T23:38:47.289125Z",
     "shell.execute_reply": "2025-04-15T23:38:47.288397Z",
     "shell.execute_reply.started": "2025-04-15T23:38:47.151159Z"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "aborted",
     "timestamp": 1744827349387,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "9ICw_lSV1bz_"
   },
   "outputs": [],
   "source": [
    "pd.Series(F.softmax(model.timesteps_attn.weights, 0).detach()).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:39:34.419446Z",
     "iopub.status.busy": "2025-04-15T23:39:34.418759Z",
     "iopub.status.idle": "2025-04-15T23:39:34.619900Z",
     "shell.execute_reply": "2025-04-15T23:39:34.619033Z",
     "shell.execute_reply.started": "2025-04-15T23:39:34.419415Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1744827349389,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "jd9XROu81b0A"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.matshow(F.softmax((model.stock_attn.weight + model.stock_attn.bias[None, :]).detach(), -1), fignum=0, norm=mpl.colors.PowerNorm(0.5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:39:35.399206Z",
     "iopub.status.busy": "2025-04-15T23:39:35.398605Z",
     "iopub.status.idle": "2025-04-15T23:39:35.681910Z",
     "shell.execute_reply": "2025-04-15T23:39:35.681047Z",
     "shell.execute_reply.started": "2025-04-15T23:39:35.399175Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1744827349402,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "nV0PwrwW1b0A"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.matshow(model.stock_emb.weight.detach(), fignum=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T17:59:25.530545Z",
     "iopub.status.busy": "2025-03-26T17:59:25.529634Z",
     "iopub.status.idle": "2025-03-26T17:59:27.665585Z",
     "shell.execute_reply": "2025-03-26T17:59:27.664393Z",
     "shell.execute_reply.started": "2025-03-26T17:59:25.530496Z"
    },
    "executionInfo": {
     "elapsed": 20302,
     "status": "aborted",
     "timestamp": 1744827349402,
     "user": {
      "displayName": "Atheesh Krishnan",
      "userId": "12070118553506761681"
     },
     "user_tz": 240
    },
    "id": "IwnDnw481b0A"
   },
   "outputs": [],
   "source": [
    "# Removed disk-based loading of train.npy\n",
    "# !rm /kaggle/working/train.npy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 2344753,
     "sourceId": 27233,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30140,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
