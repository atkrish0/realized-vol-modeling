{"cells":[{"cell_type":"markdown","id":"e5d3591f","metadata":{"papermill":{"duration":0.004842,"end_time":"2021-07-27T13:43:06.175221","exception":false,"start_time":"2021-07-27T13:43:06.170379","status":"completed"},"tags":[],"id":"e5d3591f"},"source":["This is just another lgbm baseline for this competition, a lot of the work, ideas and function are copied from https://www.kaggle.com/tommy1028/lightgbm-starter-with-feature-engineering-idea\n","\n","The features that improve the model are the aggregartions stats using time_id and stock_id using the realized volatility for different windows from the past\n","\n","Cheers and have fun"]},{"cell_type":"code","execution_count":3,"id":"5d17422a","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-07-27T13:43:06.186796Z","iopub.status.busy":"2021-07-27T13:43:06.185742Z","iopub.status.idle":"2021-07-27T13:43:08.182195Z","shell.execute_reply":"2021-07-27T13:43:08.181352Z","shell.execute_reply.started":"2021-07-27T01:13:40.906845Z"},"papermill":{"duration":2.002838,"end_time":"2021-07-27T13:43:08.182355","exception":false,"start_time":"2021-07-27T13:43:06.179517","status":"completed"},"tags":[],"id":"5d17422a","executionInfo":{"status":"ok","timestamp":1745798477631,"user_tz":240,"elapsed":7,"user":{"displayName":"Atheesh Krishnan","userId":"12070118553506761681"}}},"outputs":[],"source":["import os\n","import glob\n","from joblib import Parallel, delayed\n","import pandas as pd\n","import numpy as np\n","import scipy as sc\n","from sklearn.model_selection import KFold\n","import lightgbm as lgb\n","import warnings\n","warnings.filterwarnings('ignore')\n","#pd.set_option('max_columns', 300)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/RBS DL 2025/PRO/data'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8ejyk00sfzD","executionInfo":{"status":"ok","timestamp":1745798497707,"user_tz":240,"elapsed":15144,"user":{"displayName":"Atheesh Krishnan","userId":"12070118553506761681"}},"outputId":"765d445f-9a21-4788-b905-eaaac98e9f1e"},"id":"o8ejyk00sfzD","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":7,"id":"dc464815","metadata":{"execution":{"iopub.execute_input":"2021-07-27T13:43:08.344945Z","iopub.status.busy":"2021-07-27T13:43:08.324396Z","iopub.status.idle":"2021-07-27T14:07:17.147836Z","shell.execute_reply":"2021-07-27T14:07:17.148797Z","shell.execute_reply.started":"2021-07-27T01:13:59.221125Z"},"papermill":{"duration":1448.962151,"end_time":"2021-07-27T14:07:17.149275","exception":false,"start_time":"2021-07-27T13:43:08.187124","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dc464815","executionInfo":{"status":"error","timestamp":1745798617557,"user_tz":240,"elapsed":4565,"user":{"displayName":"Atheesh Krishnan","userId":"12070118553506761681"}},"outputId":"5c6a4349-0654-4708-8be5-f25937427317"},"outputs":[{"output_type":"stream","name":"stdout","text":["Our training set has 428932 rows\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"]},{"output_type":"error","ename":"TypeError","evalue":"incompatible index of inserted column with frame index","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)","\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 12687, in _reindex_for_setitem\n    reindexed_value = value.reindex(index)._values\n                      ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\", line 5153, in reindex\n    return super().reindex(\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\", line 5610, in reindex\n    return self._reindex_axes(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\", line 5633, in _reindex_axes\n    new_index, indexer = ax.reindex(\n                         ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\", line 4433, in reindex\n    target = self._wrap_reindex_result(target, indexer, preserve_names)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/multi.py\", line 2717, in _wrap_reindex_result\n    target = MultiIndex.from_tuples(target)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/multi.py\", line 222, in new_meth\n    return meth(self_or_cls, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/multi.py\", line 617, in from_tuples\n    arrays = list(lib.tuples_to_object_array(tuples).T)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"lib.pyx\", line 3029, in pandas._libs.lib.tuples_to_object_array\nValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-7-c8726a42631e>\", line 182, in for_joblib\n  File \"<ipython-input-7-c8726a42631e>\", line 44, in book_preprocessor\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 4311, in __setitem__\n    self._set_item(key, value)\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 4524, in _set_item\n    value, refs = self._sanitize_column(value)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 5263, in _sanitize_column\n    return _reindex_for_setitem(value, self.index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 12694, in _reindex_for_setitem\n    raise TypeError(\nTypeError: incompatible index of inserted column with frame index\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-c8726a42631e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0mtrain_stock_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stock_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;31m# Preprocess them using Parallel and our single stock id functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m \u001b[0mtrain_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_stock_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'row_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-c8726a42631e>\u001b[0m in \u001b[0;36mpreprocessor\u001b[0;34m(list_stock_ids, is_train)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Use parallel api to call paralle for loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfor_joblib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstock_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_stock_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0;31m# Concatenate all the dataframes that return from Parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1755\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: incompatible index of inserted column with frame index"]}],"source":["# data directory\n","# data_dir = '../input/optiver-realized-volatility-prediction/'\n","\n","# Function to calculate first WAP\n","def calc_wap1(df):\n","    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n","    return wap\n","\n","# Function to calculate second WAP\n","def calc_wap2(df):\n","    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n","    return wap\n","\n","# Function to calculate the log of the return\n","# Remember that logb(x / y) = logb(x) - logb(y)\n","def log_return(series):\n","    return np.log(series).diff()\n","\n","# Calculate the realized volatility\n","def realized_volatility(series):\n","    return np.sqrt(np.sum(series**2))\n","\n","# Function to count unique elements of a series\n","def count_unique(series):\n","    return len(np.unique(series))\n","\n","# Function to read our base train and test set\n","def read_train_test():\n","    train = pd.read_csv(f'{data_dir}/train.csv')\n","    test = pd.read_csv(f'{data_dir}/test.csv')\n","    # Create a key to merge with book and trade data\n","    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n","    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n","    print(f'Our training set has {train.shape[0]} rows')\n","    return train, test\n","\n","# Function to preprocess book data (for each stock id)\n","def book_preprocessor(file_path):\n","    df = pd.read_parquet(file_path)\n","    # Calculate Wap\n","    df['wap1'] = calc_wap1(df)\n","    df['wap2'] = calc_wap2(df)\n","    # Calculate log returns\n","    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n","    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n","    # Calculate wap balance\n","    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n","    # Calculate spread\n","    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n","    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n","    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n","    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n","    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n","\n","    # Dict for aggregations\n","    create_feature_dict = {\n","        'wap1': [np.sum, np.mean, np.std],\n","        'wap2': [np.sum, np.mean, np.std],\n","        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n","        'wap_balance': [np.sum, np.mean, np.std],\n","        'price_spread':[np.sum, np.mean, np.std],\n","        'bid_spread':[np.sum, np.mean, np.std],\n","        'ask_spread':[np.sum, np.mean, np.std],\n","        'total_volume':[np.sum, np.mean, np.std],\n","        'volume_imbalance':[np.sum, np.mean, np.std]\n","    }\n","\n","    # Function to get group stats for different windows (seconds in bucket)\n","    def get_stats_window(seconds_in_bucket, add_suffix = False):\n","        # Group by the window\n","        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n","        # Rename columns joining suffix\n","        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n","        # Add a suffix to differentiate windows\n","        if add_suffix:\n","            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n","        return df_feature\n","\n","    # Get the stats for different windows\n","    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n","    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n","    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n","    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n","\n","    # Merge all\n","    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n","    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n","    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n","    # Drop unnecesary time_ids\n","    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n","\n","    # Create row_id so we can merge\n","    stock_id = file_path.split('=')[1]\n","    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n","    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n","    return df_feature\n","\n","# Function to preprocess trade data (for each stock id)\n","def trade_preprocessor(file_path):\n","    df = pd.read_parquet(file_path)\n","    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n","\n","    # Dict for aggregations\n","    create_feature_dict = {\n","        'log_return':[realized_volatility],\n","        'seconds_in_bucket':[count_unique],\n","        'size':[np.sum],\n","        'order_count':[np.mean],\n","    }\n","\n","    # Function to get group stats for different windows (seconds in bucket)\n","    def get_stats_window(seconds_in_bucket, add_suffix = False):\n","        # Group by the window\n","        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n","        # Rename columns joining suffix\n","        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n","        # Add a suffix to differentiate windows\n","        if add_suffix:\n","            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n","        return df_feature\n","\n","    # Get the stats for different windows\n","    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n","    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n","    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n","    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n","\n","    # Merge all\n","    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n","    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n","    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n","    # Drop unnecesary time_ids\n","    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n","\n","    df_feature = df_feature.add_prefix('trade_')\n","    stock_id = file_path.split('=')[1]\n","    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n","    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n","    return df_feature\n","\n","# Function to get group stats for the stock_id and time_id\n","def get_time_stock(df):\n","    # Get realized volatility columns\n","    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450',\n","                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150',\n","                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n","\n","    # Group by the stock id\n","    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n","    # Rename columns joining suffix\n","    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n","    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n","\n","    # Group by the stock id\n","    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n","    # Rename columns joining suffix\n","    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n","    df_time_id = df_time_id.add_suffix('_' + 'time')\n","\n","    # Merge with original dataframe\n","    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n","    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n","    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n","    return df\n","\n","# Funtion to make preprocessing function in parallel (for each stock id)\n","def preprocessor(list_stock_ids, is_train = True):\n","\n","    # Parrallel for loop\n","    def for_joblib(stock_id):\n","        # Train\n","        if is_train:\n","            file_path_book = data_dir + \"/book_train.parquet/stock_id=\" + str(stock_id)\n","            file_path_trade = data_dir + \"/trade_train.parquet/stock_id=\" + str(stock_id)\n","        # Test\n","        else:\n","            file_path_book = data_dir + \"/book_test.parquet/stock_id=\" + str(stock_id)\n","            file_path_trade = data_dir + \"/trade_test.parquet/stock_id=\" + str(stock_id)\n","\n","        # Preprocess book and trade data and merge them\n","        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n","\n","        # Return the merge dataframe\n","        return df_tmp\n","\n","    # Use parallel api to call paralle for loop\n","    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n","    # Concatenate all the dataframes that return from Parallel\n","    df = pd.concat(df, ignore_index = True)\n","    return df\n","\n","# Function to calculate the root mean squared percentage error\n","def rmspe(y_true, y_pred):\n","    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n","\n","# Function to early stop with root mean squared percentage error\n","def feval_rmspe(y_pred, lgb_train):\n","    y_true = lgb_train.get_label()\n","    return 'RMSPE', rmspe(y_true, y_pred), False\n","\n","def train_and_evaluate(train, test):\n","    # Hyperparammeters (just basic)\n","    params = {\n","      'objective': 'rmse',\n","      'boosting_type': 'gbdt',\n","      'num_leaves': 100,\n","      'n_jobs': -1,\n","      'learning_rate': 0.1,\n","      'feature_fraction': 0.8,\n","      'bagging_fraction': 0.8,\n","      'verbose': -1\n","    }\n","\n","    # Split features and target\n","    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n","    y = train['target']\n","    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n","    # Transform stock id to a numeric value\n","    x['stock_id'] = x['stock_id'].astype(int)\n","    x_test['stock_id'] = x_test['stock_id'].astype(int)\n","\n","    # Create out of folds array\n","    oof_predictions = np.zeros(x.shape[0])\n","    # Create test array to store predictions\n","    test_predictions = np.zeros(x_test.shape[0])\n","    # Create a KFold object\n","    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n","    # Iterate through each fold\n","    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n","        print(f'Training fold {fold + 1}')\n","        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n","        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n","        # Root mean squared percentage error weights\n","        train_weights = 1 / np.square(y_train)\n","        val_weights = 1 / np.square(y_val)\n","        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n","        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n","        model = lgb.train(params = params,\n","                          train_set = train_dataset,\n","                          valid_sets = [train_dataset, val_dataset],\n","                          num_boost_round = 10000,\n","                          early_stopping_rounds = 50,\n","                          verbose_eval = 50,\n","                          feval = feval_rmspe)\n","        # Add predictions to the out of folds array\n","        oof_predictions[val_ind] = model.predict(x_val)\n","        # Predict the test set\n","        test_predictions += model.predict(x_test) / 5\n","\n","    rmspe_score = rmspe(y, oof_predictions)\n","    print(f'Our out of folds RMSPE is {rmspe_score}')\n","    # Return test predictions\n","    return test_predictions\n","\n","# Read train and test\n","train, test = read_train_test()\n","\n","# Get unique stock ids\n","train_stock_ids = train['stock_id'].unique()\n","# Preprocess them using Parallel and our single stock id functions\n","train_ = preprocessor(train_stock_ids, is_train = True)\n","train = train.merge(train_, on = ['row_id'], how = 'left')\n","\n","# Get unique stock ids\n","test_stock_ids = test['stock_id'].unique()\n","# Preprocess them using Parallel and our single stock id functions\n","test_ = preprocessor(test_stock_ids, is_train = False)\n","test = test.merge(test_, on = ['row_id'], how = 'left')\n","\n","# Get group stats of time_id and stock_id\n","train = get_time_stock(train)\n","test = get_time_stock(test)\n","\n","# Traing and evaluate\n","test_predictions = train_and_evaluate(train, test)\n","# Save test predictions\n","test['target'] = test_predictions\n","test[['row_id', 'target']].to_csv('submission.csv',index = False)"]},{"cell_type":"code","source":[],"metadata":{"id":"JoiYfh7otFIg"},"id":"JoiYfh7otFIg","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":1459.064881,"end_time":"2021-07-27T14:07:17.982008","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-07-27T13:42:58.917127","version":"2.3.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}